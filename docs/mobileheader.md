
<!DOCTYPE html><html lang="en" class="__variable_3a0388 __variable_c1e5c9"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover"/><link rel="preload" href="/_next/static/media/66f30814ff6d7cdf.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/e11418ac562b8ac1-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/43b46af2bf4eb887.css?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-6ad7d3ecd813f4e1.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T"/><script src="/_next/static/chunks/ff8e905a-855c43650bc96b2f.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T" async=""></script><script src="/_next/static/chunks/1011-d36606aa2f7c41dc.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T" async=""></script><script src="/_next/static/chunks/main-app-eeb9c06793fc7de3.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T" async=""></script><script src="/_next/static/chunks/9463-9dce0438f18c471d.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T" async=""></script><script src="/_next/static/chunks/2171-92b070254d4bb93d.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T" async=""></script><script src="/_next/static/chunks/1861-28d6a5d85b547821.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T" async=""></script><script src="/_next/static/chunks/7178-938beed2b0e105b9.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T" async=""></script><script src="/_next/static/chunks/app/layout-3d8b3fbd1021870a.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T" async=""></script><script src="/_next/static/chunks/app/llms/error-17eeb7d4f28f8bb4.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T" async=""></script><script src="/_next/static/chunks/2948-e210aa2f79594471.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T" async=""></script><script src="/_next/static/chunks/2633-9d612d2b8ec82805.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T" async=""></script><script src="/_next/static/chunks/app/llms/page-e5f508483ee9b19a.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T" async=""></script><meta name="next-size-adjust" content=""/><title>LLM Benchmark Explorer | Deploybase</title><meta name="description" content="Filter and benchmark large language models by latency, throughput, modality support, and pricing using our interactive table experience."/><meta name="format-detection" content="telephone=no, address=no, email=no"/><meta property="og:title" content="LLM Benchmark Explorer | Deploybase"/><meta property="og:description" content="Filter and benchmark large language models by latency, throughput, modality support, and pricing using our interactive table experience."/><meta property="og:url" content="https://codex-mu-eight.vercel.app/llms"/><meta property="og:image" content="https://codex-mu-eight.vercel.app/assets/data-table-infinite.png"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="LLM Benchmark Explorer | Deploybase"/><meta name="twitter:description" content="Filter and benchmark large language models by latency, throughput, modality support, and pricing using our interactive table experience."/><meta name="twitter:image" content="https://codex-mu-eight.vercel.app/assets/data-table-infinite.png"/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="31x32"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T" noModule=""></script></head><body class="min-h-[100dvh] bg-background sm:bg-muted/40 dark:sm:bg-background antialiased overscroll-x-none"><script>((e,t,r,n,o,a,i,s)=>{let u=document.documentElement,l=["light","dark"];function c(t){(Array.isArray(e)?e:[e]).forEach(e=>{let r="class"===e,n=r&&a?o.map(e=>a[e]||e):o;r?(u.classList.remove(...n),u.classList.add(t)):u.setAttribute(e,t)}),s&&l.includes(t)&&(u.style.colorScheme=t)}if(n)c(n);else try{let e=localStorage.getItem(t)||r,n=i&&"system"===e?window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light":e;c(n)}catch(e){}})("class","theme","system",null,["light","dark"],null,true,true)</script><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--><script src="/_next/static/chunks/webpack-6ad7d3ecd813f4e1.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[51850,[\"9463\",\"static/chunks/9463-9dce0438f18c471d.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"2171\",\"static/chunks/2171-92b070254d4bb93d.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"1861\",\"static/chunks/1861-28d6a5d85b547821.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"7178\",\"static/chunks/7178-938beed2b0e105b9.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"7177\",\"static/chunks/app/layout-3d8b3fbd1021870a.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\"],\"AuthProvider\"]\n3:I[59880,[\"9463\",\"static/chunks/9463-9dce0438f18c471d.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"2171\",\"static/chunks/2171-92b070254d4bb93d.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"1861\",\"static/chunks/1861-28d6a5d85b547821.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"7178\",\"static/chunks/7178-938beed2b0e105b9.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"7177\",\"static/chunks/app/layout-3d8b3fbd1021870a.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\"],\"ReactQueryProvider\"]\n4:I[12337,[\"9463\",\"static/chunks/9463-9dce0438f18c471d.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"2171\",\"static/chunks/2171-92b070254d4bb93d.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"1861\",\"static/chunks/1861-28d6a5d85b547821.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"7178\",\"static/chunks/7178-938beed2b0e105b9.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"7177\",\"static/chunks/app/layout-3d8b3fbd1021870a.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\"],\"NuqsAdapter\"]\n5:I[97643,[\"9463\",\"static/chunks/9463-9dce0438f18c471d.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"2171\",\"static/chunks/2171-92b070254d4bb93d.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"1861\",\"static/chunks/1861-28d6a5d85b547821.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"7178\",\"static/chunks/7178-938beed2b0e105b9.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"7177\",\"static/chunks/app/layout-3d8b3fbd1021870a.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\"],\"ThemeProvider\"]\n6:\"$Sreact.suspense\"\n7:I[97178,[\"9463\",\"static/chunks/9463-9dce0438f18c471d.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"2171\",\"static/chunks/2171-92b070254d4bb93d.js?dpl=dpl_72ZqLy2nteWuW"])</script><script>self.__next_f.push([1,"YsjmVQnddCvNB7T\",\"1861\",\"static/chunks/1861-28d6a5d85b547821.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"7178\",\"static/chunks/7178-938beed2b0e105b9.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"7177\",\"static/chunks/app/layout-3d8b3fbd1021870a.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\"],\"AuthDialogProvider\"]\n8:I[55950,[],\"\"]\n9:I[63844,[],\"\"]\na:I[23404,[\"9463\",\"static/chunks/9463-9dce0438f18c471d.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"5520\",\"static/chunks/app/llms/error-17eeb7d4f28f8bb4.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\"],\"default\"]\nc:I[32271,[],\"OutletBoundary\"]\ne:I[32271,[],\"MetadataBoundary\"]\n10:I[32271,[],\"ViewportBoundary\"]\n12:I[17993,[],\"\"]\n:HL[\"/_next/static/media/66f30814ff6d7cdf.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/e11418ac562b8ac1-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/css/43b46af2bf4eb887.css?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"t7XwoJ5cCop7qwAS8n_of\",\"p\":\"\",\"c\":[\"\",\"llms\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"llms\",{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/43b46af2bf4eb887.css?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"__variable_3a0388 __variable_c1e5c9\",\"suppressHydrationWarning\":true,\"children\":[\"$\",\"body\",null,{\"className\":\"min-h-[100dvh] bg-background sm:bg-muted/40 dark:sm:bg-background antialiased overscroll-x-none\",\"children\":[\"$\",\"$L2\",null,{\"children\":[\"$\",\"$L3\",null,{\"children\":[\"$\",\"$L4\",null,{\"children\":[\"$\",\"$L5\",null,{\"attribute\":\"class\",\"defaultTheme\":\"system\",\"enableSystem\":true,\"disableTransitionOnChange\":true,\"children\":[\"$\",\"$6\",null,{\"fallback\":null,\"children\":[\"$\",\"$L7\",null,{\"children\":[\"$\",\"main\",null,{\"id\":\"content\",\"className\":\"flex min-h-[100dvh] flex-col\",\"children\":[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L9\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[],[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]}]}]}]}]}]}]}]]}],{\"children\":[\"llms\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"llms\",\"children\"],\"error\":\"$a\",\"errorStyles\":[],\"errorScripts\":[],\"template\":[\"$\",\"$L9\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[],[\"$\",\"div\",null,{\"className\":\"flex min-h-dvh w-full flex-col items-center justify-center gap-4 text-center\",\"children\":[[\"$\",\"div\",null,{\"className\":\"space-y-2\",\"children\":[[\"$\",\"p\",null,{\"className\":\"text-sm uppercase tracking-wide text-muted-foreground\",\"children\":\"Not Found\"}],[\"$\",\"h1\",null,{\"className\":\"text-2xl font-semibold\",\"children\":\"Unable to locate that models view.\"}],[\"$\",\"p\",null,{\"className\":\"text-muted-foreground\",\"children\":\"Double-check the URL or head back to the LLM comparison table.\"}]]}],[\"$\",\"a\",null,{\"href\":\"/llms\",\"className\":\"inline-flex items-center rounded-md border border-border/60 bg-background px-4 py-2 text-sm font-medium text-foreground shadow-sm transition hover:bg-muted\",\"children\":\"Return to LLM Explorer\"}]]}]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$Lb\",null,[\"$\",\"$Lc\",null,{\"children\":\"$Ld\"}]]}],{},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"tVTvWjpTTxx5fxehsrNUO\",{\"children\":[[\"$\",\"$Le\",null,{\"children\":\"$Lf\"}],[\"$\",\"$L10\",null,{\"children\":\"$L11\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$12\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"11:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1, viewport-fit=cover\"}]]\n"])</script><script>self.__next_f.push([1,"f:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"1\",{\"children\":\"LLM Benchmark Explorer | Deploybase\"}],[\"$\",\"meta\",\"2\",{\"name\":\"description\",\"content\":\"Filter and benchmark large language models by latency, throughput, modality support, and pricing using our interactive table experience.\"}],[\"$\",\"meta\",\"3\",{\"name\":\"format-detection\",\"content\":\"telephone=no, address=no, email=no\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:title\",\"content\":\"LLM Benchmark Explorer | Deploybase\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:description\",\"content\":\"Filter and benchmark large language models by latency, throughput, modality support, and pricing using our interactive table experience.\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:url\",\"content\":\"https://codex-mu-eight.vercel.app/llms\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:image\",\"content\":\"https://codex-mu-eight.vercel.app/assets/data-table-infinite.png\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"9\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"10\",{\"name\":\"twitter:title\",\"content\":\"LLM Benchmark Explorer | Deploybase\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:description\",\"content\":\"Filter and benchmark large language models by latency, throughput, modality support, and pricing using our interactive table experience.\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:image\",\"content\":\"https://codex-mu-eight.vercel.app/assets/data-table-infinite.png\"}],[\"$\",\"link\",\"13\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"31x32\"}]]\n"])</script><script>self.__next_f.push([1,"d:null\n"])</script><script>self.__next_f.push([1,"13:I[64409,[\"9463\",\"static/chunks/9463-9dce0438f18c471d.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"2171\",\"static/chunks/2171-92b070254d4bb93d.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"1861\",\"static/chunks/1861-28d6a5d85b547821.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"2948\",\"static/chunks/2948-e210aa2f79594471.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"7178\",\"static/chunks/7178-938beed2b0e105b9.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"2633\",\"static/chunks/2633-9d612d2b8ec82805.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"2791\",\"static/chunks/app/llms/page-e5f508483ee9b19a.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\"],\"HydrationBoundary\"]\n19:I[52607,[\"9463\",\"static/chunks/9463-9dce0438f18c471d.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"2171\",\"static/chunks/2171-92b070254d4bb93d.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"1861\",\"static/chunks/1861-28d6a5d85b547821.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"2948\",\"static/chunks/2948-e210aa2f79594471.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"7178\",\"static/chunks/7178-938beed2b0e105b9.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"2633\",\"static/chunks/2633-9d612d2b8ec82805.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\",\"2791\",\"static/chunks/app/llms/page-e5f508483ee9b19a.js?dpl=dpl_72ZqLy2nteWuWYsjmVQnddCvNB7T\"],\"ModelsClient\"]\n14:T415,Qwen3-Next-80B-A3B-Instruct is an instruction-tuned chat model in the Qwen3-Next series optimized for fast, stable responses without “thinking” traces. It targets complex tasks across reasoning, code generation, knowledge QA, and multilingual use, while remaining robust on alignment and formatting. Compared with prior Qwen3 instruct variants, it focuses on higher throughput and stability on ultra-long inputs and multi-turn dialogues, making it well-suited for RAG, tool use, and agentic workflows that require consistent final answers rather than visible chain-of-thought.\n\nThe model employs scaling-efficient training and decoding to improve parameter efficiency and inference speed, and has been validated on a broad set of public benchmarks where it reaches "])</script><script>self.__next_f.push([1,"or approaches larger Qwen3 systems in several categories while outperforming earlier mid-sized baselines. It is best used as a general assistant, code helper, and long-context task solver in production settings where deterministic, instruction-following outputs are preferred.15:T4aa,Qwen3-VL-235B-A22B Instruct is an open-weight multimodal model that unifies strong text generation with visual understanding across images and video. The Instruct model targets general vision-language use (VQA, document parsing, chart/table extraction, multilingual OCR). The series emphasizes robust perception (recognition of diverse real-world and synthetic categories), spatial understanding (2D/3D grounding), and long-form visual comprehension, with competitive results on public multimodal benchmarks for both perception and reasoning.\n\nBeyond analysis, Qwen3-VL supports agentic interaction and tool use: it can follow complex instructions over multi-image, multi-turn dialogues; align text to video timelines for precise temporal queries; and operate GUI elements for automation tasks. The models also enable visual coding workflows—turning sketches or mockups into code and assisting with UI debugging—while maintaining strong text-only performance comparable to the flagship Qwen3 language models. This makes Qwen3-VL suitable for production scenarios spanning document AI, multilingual OCR, software/UI assistance, spatial/embodied tasks, and research on vision-language agents.16:T422,Claude Opus 4.5 is Anthropic’s frontier reasoning model optimized for complex software engineering, agentic workflows, and long-horizon computer use. It offers strong multimodal capabilities, competitive performance across real-world coding and reasoning benchmarks, and improved robustness to prompt injection. The model is designed to operate efficiently across varied effort levels, enabling developers to trade off speed, depth, and token usage depending on task requirements. It comes with a new parameter to control token efficiency, which can be accesse"])</script><script>self.__next_f.push([1,"d using the OpenRouter Verbosity parameter with low, medium, or high.\n\nOpus 4.5 supports advanced tool use, extended context management, and coordinated multi-agent setups, making it well-suited for autonomous research, debugging, multi-step planning, and spreadsheet/browser manipulation. It delivers substantial gains in structured reasoning, execution reliability, and alignment compared to prior Opus generations, while reducing token overhead and improving performance on long-running tasks.17:T422,Claude Opus 4.5 is Anthropic’s frontier reasoning model optimized for complex software engineering, agentic workflows, and long-horizon computer use. It offers strong multimodal capabilities, competitive performance across real-world coding and reasoning benchmarks, and improved robustness to prompt injection. The model is designed to operate efficiently across varied effort levels, enabling developers to trade off speed, depth, and token usage depending on task requirements. It comes with a new parameter to control token efficiency, which can be accessed using the OpenRouter Verbosity parameter with low, medium, or high.\n\nOpus 4.5 supports advanced tool use, extended context management, and coordinated multi-agent setups, making it well-suited for autonomous research, debugging, multi-step planning, and spreadsheet/browser manipulation. It delivers substantial gains in structured reasoning, execution reliability, and alignment compared to prior Opus generations, while reducing token overhead and improving performance on long-running tasks.18:T130d5,"])</script><script>self.__next_f.push([1,"{\"@context\":\"https://schema.org\",\"@type\":\"DataFeed\",\"name\":\"LLM Benchmark Feed\",\"dateModified\":\"2025-11-28T14:39:45.585Z\",\"dataFeedElement\":[{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:21.583Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"AI21: Jamba Large 1.7\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Jamba Large 1.7 is the latest model in the Jamba open family, offering improvements in grounding, instruction-following, and overall efficiency. Built on a hybrid SSM-Transformer architecture with a 256K context window, it delivers more accurate, contextually grounded responses and better steerability than previous versions.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"AI21\"},\"author\":{\"@type\":\"Organization\",\"name\":\"AI21\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":256000},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"ai21/ai21/jamba-large-1.7--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":2},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":8},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":34.27}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=ai21/ai21/jamba-large-1.7--variant-standard--seq-2\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:21.583Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"AI21: Jamba Mini 1.7\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Jamba Mini 1.7 is a compact and efficient member of the Jamba open model family, incorporating key improvements in grounding and instruction-following while maintaining the benefits of the SSM-Transformer hybrid architecture and 256K context window. Despite its compact size, it delivers accurate, contextually grounded responses and improved steerability.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"AI21\"},\"author\":{\"@type\":\"Organization\",\"name\":\"AI21\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":256000},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"ai21/ai21/jamba-mini-1.7--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":0.2},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":0.4},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":168.27}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=ai21/ai21/jamba-mini-1.7--variant-standard--seq-1\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:21.780Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"AionLabs: Aion-1.0\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Aion-1.0 is a multi-model system designed for high performance across various tasks, including reasoning and coding. It is built on DeepSeek-R1, augmented with additional models and techniques such as Tree of Thoughts (ToT) and Mixture of Experts (MoE). It is Aion Lab's most powerful reasoning model.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"AionLabs\"},\"author\":{\"@type\":\"Organization\",\"name\":\"AionLabs\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":131072},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"aionlabs/aion-labs/aion-1.0--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":4},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":8},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":61.07}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=aionlabs/aion-labs/aion-1.0--variant-standard--seq-3\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:21.780Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"AionLabs: Aion-1.0-Mini\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Aion-1.0-Mini 32B parameter model is a distilled version of the DeepSeek-R1 model, designed for strong performance in reasoning domains such as mathematics, coding, and logic. It is a modified variant of a FuseAI model that outperforms R1-Distill-Qwen-32B and R1-Distill-Llama-70B, with benchmark results available on its [Hugging Face page](https://huggingface.co/FuseAI/FuseO1-DeepSeekR1-QwQ-SkyT1-32B-Preview), independently replicated for verification.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"AionLabs\"},\"author\":{\"@type\":\"Organization\",\"name\":\"AionLabs\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":131072},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"aionlabs/aion-labs/aion-1.0-mini--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":0.7},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":1.4},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":184.57}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=aionlabs/aion-labs/aion-1.0-mini--variant-standard--seq-4\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:21.780Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"AionLabs: Aion-RP 1.0 (8B)\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Aion-RP-Llama-3.1-8B ranks the highest in the character evaluation portion of the RPBench-Auto benchmark, a roleplaying-specific variant of Arena-Hard-Auto, where LLMs evaluate each other’s responses. It is a fine-tuned base model rather than an instruct model, designed to produce more natural and varied writing.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"AionLabs\"},\"author\":{\"@type\":\"Organization\",\"name\":\"AionLabs\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":32768},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"aionlabs/aion-labs/aion-rp-llama-3.1-8b--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":0.2},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":0.2},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":44.57}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=aionlabs/aion-labs/aion-rp-llama-3.1-8b--variant-standard--seq-5\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:21.983Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Qwen: Qwen3 235B A22B Instruct 2507\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Qwen3-235B-A22B-Instruct-2507 is a multilingual, instruction-tuned mixture-of-experts language model based on the Qwen3-235B architecture, with 22B active parameters per forward pass. It is optimized for general-purpose text generation, including instruction following, logical reasoning, math, code, and tool usage. The model supports a native 262K context length and does not implement \\\"thinking mode\\\" (\\u003cthink\u003e blocks).\\n\\nCompared to its base variant, this version delivers significant gains in knowledge coverage, long-context reasoning, coding benchmarks, and alignment with open-ended tasks. It is particularly strong on multilingual understanding, math reasoning (e.g., AIME, HMMT), and alignment evaluations like Arena-Hard and WritingBench.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Alibaba\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Qwen\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":131072},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"alibaba/qwen/qwen3-235b-a22b-2507--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":0.23},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":0.92},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":85.53}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=alibaba/qwen/qwen3-235b-a22b-2507--variant-standard--seq-20\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:21.983Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Qwen: Qwen3 30B A3B Instruct 2507\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Qwen3-30B-A3B-Instruct-2507 is a 30.5B-parameter mixture-of-experts language model from Qwen, with 3.3B active parameters per inference. It operates in non-thinking mode and is designed for high-quality instruction following, multilingual understanding, and agentic tool use. Post-trained on instruction data, it demonstrates competitive performance across reasoning (AIME, ZebraLogic), coding (MultiPL-E, LiveCodeBench), and alignment (IFEval, WritingBench) benchmarks. It outperforms its non-instruct variant on subjective and open-ended tasks while retaining strong factual and coding performance.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Alibaba\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Qwen\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":131072},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"alibaba/qwen/qwen3-30b-a3b-instruct-2507--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":0.2},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":0.8},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":79.57}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=alibaba/qwen/qwen3-30b-a3b-instruct-2507--variant-standard--seq-18\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:21.983Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Qwen: Qwen3 30B A3B Thinking 2507\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Qwen3-30B-A3B-Thinking-2507 is a 30B parameter Mixture-of-Experts reasoning model optimized for complex tasks requiring extended multi-step thinking. The model is designed specifically for “thinking mode,” where internal reasoning traces are separated from final answers.\\n\\nCompared to earlier Qwen3-30B releases, this version improves performance across logical reasoning, mathematics, science, coding, and multilingual benchmarks. It also demonstrates stronger instruction following, tool use, and alignment with human preferences. With higher reasoning efficiency and extended output budgets, it is best suited for advanced research, competitive problem solving, and agentic applications requiring structured long-context reasoning.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Alibaba\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Qwen\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":131072},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"alibaba/qwen/qwen3-30b-a3b-thinking-2507--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":0.2},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":2.4},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":155.23}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=alibaba/qwen/qwen3-30b-a3b-thinking-2507--variant-standard--seq-16\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:21.983Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Qwen: Qwen3 Coder 30B A3B Instruct\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Qwen3-Coder-30B-A3B-Instruct is a 30.5B parameter Mixture-of-Experts (MoE) model with 128 experts (8 active per forward pass), designed for advanced code generation, repository-scale understanding, and agentic tool use. Built on the Qwen3 architecture, it supports a native context length of 256K tokens (extendable to 1M with Yarn) and performs strongly in tasks involving function calls, browser use, and structured code completion.\\n\\nThis model is optimized for instruction-following without “thinking mode”, and integrates well with OpenAI-compatible tool-use formats. \",\"provider\":{\"@type\":\"Organization\",\"name\":\"Alibaba\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Qwen\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":200000},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"alibaba/qwen/qwen3-coder-30b-a3b-instruct--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":0.75},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":3.75},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":107.14}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=alibaba/qwen/qwen3-coder-30b-a3b-instruct--variant-standard--seq-17\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:21.983Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Qwen: Qwen3 Coder 480B A35B\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) code generation model developed by the Qwen team. It is optimized for agentic coding tasks such as function calling, tool use, and long-context reasoning over repositories. The model features 480 billion total parameters, with 35 billion active per forward pass (8 out of 160 experts).\\n\\nPricing for the Alibaba endpoints varies by context length. Once a request is greater than 128k input tokens, the higher pricing is used.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Alibaba\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Qwen\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":262144},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"alibaba/qwen/qwen3-coder--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":1.5},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":7.5},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":70.08}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=alibaba/qwen/qwen3-coder--variant-standard--seq-19\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:21.983Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Qwen: Qwen3 Coder Flash\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Qwen3 Coder Flash is Alibaba's fast and cost efficient version of their proprietary Qwen3 Coder Plus. It is a powerful coding agent model specializing in autonomous programming via tool calling and environment interaction, combining coding proficiency with versatile general-purpose abilities.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Alibaba\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Qwen\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":128000},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"alibaba/qwen/qwen3-coder-flash--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":0.3},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":1.5},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":107.47}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=alibaba/qwen/qwen3-coder-flash--variant-standard--seq-11\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:21.983Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Qwen: Qwen3 Coder Plus\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Qwen3 Coder Plus is Alibaba's proprietary version of the Open Source Qwen3 Coder 480B A35B. It is a powerful coding agent model specializing in autonomous programming via tool calling and environment interaction, combining coding proficiency with versatile general-purpose abilities.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Alibaba\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Qwen\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":128000},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"alibaba/qwen/qwen3-coder-plus--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":1},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":5},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":65.66}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=alibaba/qwen/qwen3-coder-plus--variant-standard--seq-10\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:21.983Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Qwen: Qwen3 Max\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Qwen3-Max is an updated release built on the Qwen3 series, offering major improvements in reasoning, instruction following, multilingual support, and long-tail knowledge coverage compared to the January 2025 version. It delivers higher accuracy in math, coding, logic, and science tasks, follows complex instructions in Chinese and English more reliably, reduces hallucinations, and produces higher-quality responses for open-ended Q\u0026A, writing, and conversation. The model supports over 100 languages with stronger translation and commonsense reasoning, and is optimized for retrieval-augmented generation (RAG) and tool calling, though it does not include a dedicated “thinking” mode.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Alibaba\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Qwen\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":256000},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"alibaba/qwen/qwen3-max--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":1.2},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":6},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":34.36}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=alibaba/qwen/qwen3-max--variant-standard--seq-9\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:21.983Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Qwen: Qwen3 Next 80B A3B Instruct\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Qwen3-Next-80B-A3B-Instruct is an instruction-tuned chat model in the Qwen3-Next series optimized for fast, stable responses without “thinking” traces. It targets complex tasks across reasoning, code generation, knowledge QA, and multilingual use, while remaining robust on alignment and formatting. Compared with prior Qwen3 instruct variants, it focuses on higher throughput and stability on ultra-long inputs and multi-turn dialogues, making it well-suited for RAG, tool use, and agentic workflows that require consistent final answers rather than visible chain-of-thought.\\n\\nThe model employs scaling-efficient training and decoding to improve parameter efficiency and inference speed, and has been validated on a broad set of public benchmarks where it reaches or approaches larger Qwen3 systems in several categories while outperforming earlier mid-sized baselines. It is best used as a general assistant, code helper, and long-context task solver in production settings where deterministic, instruction-following outputs are preferred.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Alibaba\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Qwen\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":131072},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"alibaba/qwen/qwen3-next-80b-a3b-instruct--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":0.15},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":1.2},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":114.28}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=alibaba/qwen/qwen3-next-80b-a3b-instruct--variant-standard--seq-13\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:21.983Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Qwen: Qwen3 Next 80B A3B Thinking\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Qwen3-Next-80B-A3B-Thinking is a reasoning-first chat model in the Qwen3-Next line that outputs structured “thinking” traces by default. It’s designed for hard multi-step problems; math proofs, code synthesis/debugging, logic, and agentic planning, and reports strong results across knowledge, reasoning, coding, alignment, and multilingual evaluations. Compared with prior Qwen3 variants, it emphasizes stability under long chains of thought and efficient scaling during inference, and it is tuned to follow complex instructions while reducing repetitive or off-task behavior.\\n\\nThe model is suitable for agent frameworks and tool use (function calling), retrieval-heavy workflows, and standardized benchmarking where step-by-step solutions are required. It supports long, detailed completions and leverages throughput-oriented techniques (e.g., multi-token prediction) for faster generation. Note that it operates in thinking-only mode.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Alibaba\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Qwen\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":262144},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"alibaba/qwen/qwen3-next-80b-a3b-thinking--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":0.5},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":6},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":151.46}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=alibaba/qwen/qwen3-next-80b-a3b-thinking--variant-standard--seq-12\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:21.983Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Qwen: Qwen3 VL 235B A22B Instruct\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Qwen3-VL-235B-A22B Instruct is an open-weight multimodal model that unifies strong text generation with visual understanding across images and video. The Instruct model targets general vision-language use (VQA, document parsing, chart/table extraction, multilingual OCR). The series emphasizes robust perception (recognition of diverse real-world and synthetic categories), spatial understanding (2D/3D grounding), and long-form visual comprehension, with competitive results on public multimodal benchmarks for both perception and reasoning.\\n\\nBeyond analysis, Qwen3-VL supports agentic interaction and tool use: it can follow complex instructions over multi-image, multi-turn dialogues; align text to video timelines for precise temporal queries; and operate GUI elements for automation tasks. The models also enable visual coding workflows—turning sketches or mockups into code and assisting with UI debugging—while maintaining strong text-only performance comparable to the flagship Qwen3 language models. This makes Qwen3-VL suitable for production scenarios spanning document AI, multilingual OCR, software/UI assistance, spatial/embodied tasks, and research on vision-language agents.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Alibaba\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Qwen\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":131072},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"alibaba/qwen/qwen3-vl-235b-a22b-instruct--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text, image\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":0.4},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":1.6},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":42.13}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=alibaba/qwen/qwen3-vl-235b-a22b-instruct--variant-standard--seq-8\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:21.983Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Qwen: Qwen3 VL 8B Instruct\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Qwen3-VL-8B-Instruct is a multimodal vision-language model from the Qwen3-VL series, built for high-fidelity understanding and reasoning across text, images, and video. It features improved multimodal fusion with Interleaved-MRoPE for long-horizon temporal reasoning, DeepStack for fine-grained visual-text alignment, and text-timestamp alignment for precise event localization.\\n\\nThe model supports a native 256K-token context window, extensible to 1M tokens, and handles both static and dynamic media inputs for tasks like document parsing, visual question answering, spatial reasoning, and GUI control. It achieves text understanding comparable to leading LLMs while expanding OCR coverage to 32 languages and enhancing robustness under varied visual conditions.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Alibaba\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Qwen\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":256000},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"alibaba/qwen/qwen3-vl-8b-instruct--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"image, text\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":0.18},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":0.7},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":121.78}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=alibaba/qwen/qwen3-vl-8b-instruct--variant-standard--seq-7\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:21.983Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Qwen: Qwen3 VL 8B Thinking\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Qwen3-VL-8B-Thinking is the reasoning-optimized variant of the Qwen3-VL-8B multimodal model, designed for advanced visual and textual reasoning across complex scenes, documents, and temporal sequences. It integrates enhanced multimodal alignment and long-context processing (native 256K, expandable to 1M tokens) for tasks such as scientific visual analysis, causal inference, and mathematical reasoning over image or video inputs.\\n\\nCompared to the Instruct edition, the Thinking version introduces deeper visual-language fusion and deliberate reasoning pathways that improve performance on long-chain logic tasks, STEM problem-solving, and multi-step video understanding. It achieves stronger temporal grounding via Interleaved-MRoPE and timestamp-aware embeddings, while maintaining robust OCR, multilingual comprehension, and text generation on par with large text-only LLMs.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Alibaba\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Qwen\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":256000},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"alibaba/qwen/qwen3-vl-8b-thinking--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"image, text\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":0.18},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":2.1},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":65.73}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=alibaba/qwen/qwen3-vl-8b-thinking--variant-standard--seq-6\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:21.983Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Qwen: Qwen-Max \",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Qwen-Max, based on Qwen2.5, provides the best inference performance among [Qwen models](/qwen), especially for complex multi-step tasks. It's a large-scale MoE model that has been pretrained on over 20 trillion tokens and further post-trained with curated Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) methodologies. The parameter count is unknown.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Alibaba\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Qwen\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":32768},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"alibaba/qwen/qwen-max--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":1.6},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":6.4},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":24.38}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=alibaba/qwen/qwen-max--variant-standard--seq-25\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:21.983Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Qwen: Qwen-Plus\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Qwen-Plus, based on the Qwen2.5 foundation model, is a 131K context model with a balanced performance, speed, and cost combination.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Alibaba\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Qwen\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":131072},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"alibaba/qwen/qwen-plus--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":0.4},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":1.2},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":63.34}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=alibaba/qwen/qwen-plus--variant-standard--seq-24\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:21.983Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Qwen: Qwen Plus 0728\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Qwen Plus 0728, based on the Qwen3 foundation model, is a 1 million context hybrid reasoning model with a balanced performance, speed, and cost combination.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Alibaba\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Qwen\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":1000000},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"alibaba/qwen/qwen-plus-2025-07-28--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":0.4},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":1.2},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":64.49}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=alibaba/qwen/qwen-plus-2025-07-28--variant-standard--seq-14\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:21.983Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Qwen: Qwen Plus 0728 (thinking)\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Qwen Plus 0728, based on the Qwen3 foundation model, is a 1 million context hybrid reasoning model with a balanced performance, speed, and cost combination.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Alibaba\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Qwen\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":1000000},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"alibaba/qwen/qwen-plus-2025-07-28--variant-thinking\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":0.4},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":4},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":81.15}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=alibaba/qwen/qwen-plus-2025-07-28--variant-thinking--seq-15\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:21.983Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Qwen: Qwen-Turbo\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Qwen-Turbo, based on Qwen2.5, is a 1M context model that provides fast speed and low cost, suitable for simple tasks.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Alibaba\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Qwen\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":1000000},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"alibaba/qwen/qwen-turbo--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":0.05},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":0.2},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":58.99}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=alibaba/qwen/qwen-turbo--variant-standard--seq-23\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:21.983Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Qwen: Qwen VL Max\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Qwen VL Max is a visual understanding model with 7500 tokens context length. It excels in delivering optimal performance for a broader spectrum of complex tasks.\\n\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Alibaba\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Qwen\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":131072},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"alibaba/qwen/qwen-vl-max--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text, image\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":0.8},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":3.2},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":50.06}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=alibaba/qwen/qwen-vl-max--variant-standard--seq-22\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:21.983Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Qwen: Qwen VL Plus\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Qwen's Enhanced Large Visual Language Model. Significantly upgraded for detailed recognition capabilities and text recognition abilities, supporting ultra-high pixel resolutions up to millions of pixels and extreme aspect ratios for image input. It delivers significant performance across a broad range of visual tasks.\\n\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Alibaba\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Qwen\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":7500},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"alibaba/qwen/qwen-vl-plus--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text, image\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":0.21},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":0.63},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":102.78}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=alibaba/qwen/qwen-vl-plus--variant-standard--seq-21\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:22.173Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Anthropic: Claude 3.5 Haiku\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Claude 3.5 Haiku features offers enhanced capabilities in speed, coding accuracy, and tool use. Engineered to excel in real-time applications, it delivers quick response times that are essential for dynamic tasks such as chat interactions and immediate coding suggestions.\\n\\nThis makes it highly suitable for environments that demand both speed and precision, such as software development, customer service bots, and data management systems.\\n\\nThis model is currently pointing to [Claude 3.5 Haiku (2024-10-22)](/anthropic/claude-3-5-haiku-20241022).\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Amazon Bedrock\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Anthropic\"},\"softwareVersion\":\"028ec497-a034-40fd-81fe-f51d0a0c640c\",\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":200000},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"amazon bedrock/anthropic/claude-3.5-haiku--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text, image\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":0.8},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":4},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":67.17}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=amazon/bedrock/anthropic/claude-3.5-haiku--variant-standard--seq-40\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:22.173Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Anthropic: Claude 3.5 Sonnet\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"New Claude 3.5 Sonnet delivers better-than-Opus capabilities, faster-than-Sonnet speeds, at the same Sonnet prices. Sonnet is particularly good at:\\n\\n- Coding: Scores ~49% on SWE-Bench Verified, higher than the last best score, and without any fancy prompt scaffolding\\n- Data science: Augments human data science expertise; navigates unstructured data while using multiple tools for insights\\n- Visual processing: excelling at interpreting charts, graphs, and images, accurately transcribing text to derive insights beyond just the text alone\\n- Agentic tasks: exceptional tool use, making it great at agentic tasks (i.e. complex, multi-step problem solving tasks that require engaging with other systems)\\n\\n#multimodal\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Amazon Bedrock\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Anthropic\"},\"softwareVersion\":\"30636d20-cda3-4a59-aa0c-1a5b6efba072\",\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":200000},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"amazon bedrock/anthropic/claude-3.5-sonnet--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text, image, file\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":3},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":15},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":47.07}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=amazon/bedrock/anthropic/claude-3.5-sonnet--variant-standard--seq-41\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:22.173Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Anthropic: Claude 3.7 Sonnet\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Claude 3.7 Sonnet is an advanced large language model with improved reasoning, coding, and problem-solving capabilities. It introduces a hybrid reasoning approach, allowing users to choose between rapid responses and extended, step-by-step processing for complex tasks. The model demonstrates notable improvements in coding, particularly in front-end development and full-stack updates, and excels in agentic workflows, where it can autonomously navigate multi-step processes. \\n\\nClaude 3.7 Sonnet maintains performance parity with its predecessor in standard mode while offering an extended reasoning mode for enhanced accuracy in math, coding, and instruction-following tasks.\\n\\nRead more at the [blog post here](https://www.anthropic.com/news/claude-3-7-sonnet)\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Amazon Bedrock\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Anthropic\"},\"softwareVersion\":\"30636d20-cda3-4a59-aa0c-1a5b6efba072\",\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":200000},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"amazon bedrock/anthropic/claude-3.7-sonnet--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text, image, file\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":3},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":15},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":104.64}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=amazon/bedrock/anthropic/claude-3.7-sonnet--variant-standard--seq-36\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:22.173Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Anthropic: Claude Haiku 4.5\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Claude Haiku 4.5 is Anthropic’s fastest and most efficient model, delivering near-frontier intelligence at a fraction of the cost and latency of larger Claude models. Matching Claude Sonnet 4’s performance across reasoning, coding, and computer-use tasks, Haiku 4.5 brings frontier-level capability to real-time and high-volume applications.\\n\\nIt introduces extended thinking to the Haiku line; enabling controllable reasoning depth, summarized or interleaved thought output, and tool-assisted workflows with full support for coding, bash, web search, and computer-use tools. Scoring \u003e73% on SWE-bench Verified, Haiku 4.5 ranks among the world’s best coding models while maintaining exceptional responsiveness for sub-agents, parallelized execution, and scaled deployment.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Amazon Bedrock\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Anthropic\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":200000},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"amazon bedrock/anthropic/claude-haiku-4.5--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"image, text\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":1},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":5},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":121.37}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=amazon/bedrock/anthropic/claude-haiku-4.5--variant-standard--seq-28\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:22.173Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Anthropic: Claude Opus 4\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Claude Opus 4 is benchmarked as the world’s best coding model, at time of release, bringing sustained performance on complex, long-running tasks and agent workflows. It sets new benchmarks in software engineering, achieving leading results on SWE-bench (72.5%) and Terminal-bench (43.2%). Opus 4 supports extended, agentic workflows, handling thousands of task steps continuously for hours without degradation. \\n\\nRead more at the [blog post here](https://www.anthropic.com/news/claude-4)\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Amazon Bedrock\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Anthropic\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":200000},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"amazon bedrock/anthropic/claude-opus-4--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"image, text, file\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":15},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":75},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":20.22}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=amazon/bedrock/anthropic/claude-opus-4--variant-standard--seq-34\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:22.173Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Anthropic: Claude Opus 4.1\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Claude Opus 4.1 is an updated version of Anthropic’s flagship model, offering improved performance in coding, reasoning, and agentic tasks. It achieves 74.5% on SWE-bench Verified and shows notable gains in multi-file code refactoring, debugging precision, and detail-oriented reasoning. The model supports extended thinking up to 64K tokens and is optimized for tasks involving research, data analysis, and tool-assisted reasoning.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Amazon Bedrock\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Anthropic\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":200000},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"amazon bedrock/anthropic/claude-opus-4.1--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"image, text, file\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":15},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":75},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":17.7}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=amazon/bedrock/anthropic/claude-opus-4.1--variant-standard--seq-32\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:22.173Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Anthropic: Claude Opus 4.5\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Claude Opus 4.5 is Anthropic’s frontier reasoning model optimized for complex software engineering, agentic workflows, and long-horizon computer use. It offers strong multimodal capabilities, competitive performance across real-world coding and reasoning benchmarks, and improved robustness to prompt injection. The model is designed to operate efficiently across varied effort levels, enabling developers to trade off speed, depth, and token usage depending on task requirements. It comes with a new parameter to control token efficiency, which can be accessed using the OpenRouter Verbosity parameter with low, medium, or high.\\n\\nOpus 4.5 supports advanced tool use, extended context management, and coordinated multi-agent setups, making it well-suited for autonomous research, debugging, multi-step planning, and spreadsheet/browser manipulation. It delivers substantial gains in structured reasoning, execution reliability, and alignment compared to prior Opus generations, while reducing token overhead and improving performance on long-running tasks.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Amazon Bedrock\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Anthropic\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":200000},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"amazon bedrock/anthropic/claude-opus-4.5--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"file, image, text\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":5},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":25},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":63.56}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=amazon/bedrock/anthropic/claude-opus-4.5--variant-standard--seq-26\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:22.173Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Anthropic: Claude Sonnet 4\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Claude Sonnet 4 significantly enhances the capabilities of its predecessor, Sonnet 3.7, excelling in both coding and reasoning tasks with improved precision and controllability. Achieving state-of-the-art performance on SWE-bench (72.7%), Sonnet 4 balances capability and computational efficiency, making it suitable for a broad range of applications from routine coding tasks to complex software development projects. Key enhancements include improved autonomous codebase navigation, reduced error rates in agent-driven workflows, and increased reliability in following intricate instructions. Sonnet 4 is optimized for practical everyday use, providing advanced reasoning capabilities while maintaining efficiency and responsiveness in diverse internal and external scenarios.\\n\\nRead more at the [blog post here](https://www.anthropic.com/news/claude-4)\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Amazon Bedrock\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Anthropic\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":200000},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"amazon bedrock/anthropic/claude-sonnet-4--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"image, text, file\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":3},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":15},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":101.35}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=amazon/bedrock/anthropic/claude-sonnet-4--variant-standard--seq-35\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:22.173Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Anthropic: Claude Sonnet 4.5\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Claude Sonnet 4.5 is Anthropic’s most advanced Sonnet model to date, optimized for real-world agents and coding workflows. It delivers state-of-the-art performance on coding benchmarks such as SWE-bench Verified, with improvements across system design, code security, and specification adherence. The model is designed for extended autonomous operation, maintaining task continuity across sessions and providing fact-based progress tracking.\\n\\nSonnet 4.5 also introduces stronger agentic capabilities, including improved tool orchestration, speculative parallel execution, and more efficient context and memory management. With enhanced context tracking and awareness of token usage across tool calls, it is particularly well-suited for multi-context and long-running workflows. Use cases span software engineering, cybersecurity, financial analysis, research agents, and other domains requiring sustained reasoning and tool use.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Amazon Bedrock\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Anthropic\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":1000000},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"amazon bedrock/anthropic/claude-sonnet-4.5--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text, image, file\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":3},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":15},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":66.71}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=amazon/bedrock/anthropic/claude-sonnet-4.5--variant-standard--seq-29\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:22.173Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"OpenAI: gpt-oss-120b\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Amazon Bedrock\"},\"author\":{\"@type\":\"Organization\",\"name\":\"OpenAI\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":131072},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"amazon bedrock/openai/gpt-oss-120b--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":0.15},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":0.6},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":852.39}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=amazon/bedrock/openai/gpt-oss-120b--variant-standard--seq-30\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:22.173Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"OpenAI: gpt-oss-20b\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"gpt-oss-20b is an open-weight 21B parameter model released by OpenAI under the Apache 2.0 license. It uses a Mixture-of-Experts (MoE) architecture with 3.6B active parameters per forward pass, optimized for lower-latency inference and deployability on consumer or single-GPU hardware. The model is trained in OpenAI’s Harmony response format and supports reasoning level configuration, fine-tuning, and agentic capabilities including function calling, tool use, and structured outputs.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Amazon Bedrock\"},\"author\":{\"@type\":\"Organization\",\"name\":\"OpenAI\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":131072},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"amazon bedrock/openai/gpt-oss-20b--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":0.07},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":0.15},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":1611.11}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=amazon/bedrock/openai/gpt-oss-20b--variant-standard--seq-31\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:22.173Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Amazon: Nova Lite 1.0\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Amazon Nova Lite 1.0 is a very low-cost multimodal model from Amazon that focused on fast processing of image, video, and text inputs to generate text output. Amazon Nova Lite can handle real-time customer interactions, document analysis, and visual question-answering tasks with high accuracy.\\n\\nWith an input context of 300K tokens, it can analyze multiple images or up to 30 minutes of video in a single input.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Amazon Bedrock\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Amazon\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":300000},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"amazon bedrock/amazon/nova-lite-v1--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text, image\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":0.06},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":0.24},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":130.33}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=amazon/bedrock/amazon/nova-lite-v1--variant-standard--seq-37\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:22.173Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Amazon: Nova Micro 1.0\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Amazon Nova Micro 1.0 is a text-only model that delivers the lowest latency responses in the Amazon Nova family of models at a very low cost. With a context length of 128K tokens and optimized for speed and cost, Amazon Nova Micro excels at tasks such as text summarization, translation, content classification, interactive chat, and brainstorming. It has  simple mathematical reasoning and coding abilities.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Amazon Bedrock\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Amazon\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":128000},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"amazon bedrock/amazon/nova-micro-v1--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":0.035},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":0.14},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":361.13}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=amazon/bedrock/amazon/nova-micro-v1--variant-standard--seq-38\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:22.173Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Amazon: Nova Premier 1.0\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Amazon Nova Premier is the most capable of Amazon’s multimodal models for complex reasoning tasks and for use as the best teacher for distilling custom models.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Amazon Bedrock\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Amazon\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":1000000},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"amazon bedrock/amazon/nova-premier-v1--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text, image\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":2.5},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":12.5},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":39.9}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=amazon/bedrock/amazon/nova-premier-v1--variant-standard--seq-27\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:22.173Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Amazon: Nova Pro 1.0\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Amazon Nova Pro 1.0 is a capable multimodal model from Amazon focused on providing a combination of accuracy, speed, and cost for a wide range of tasks. As of December 2024, it achieves state-of-the-art performance on key benchmarks including visual question answering (TextVQA) and video understanding (VATEX).\\n\\nAmazon Nova Pro demonstrates strong capabilities in processing both visual and textual information and at analyzing financial documents.\\n\\n**NOTE**: Video input is not supported at this time.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Amazon Bedrock\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Amazon\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":300000},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"amazon bedrock/amazon/nova-pro-v1--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text, image\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":0.8},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":3.2},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":92.82}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=amazon/bedrock/amazon/nova-pro-v1--variant-standard--seq-39\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:22.173Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Qwen: Qwen3 Coder 30B A3B Instruct\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Qwen3-Coder-30B-A3B-Instruct is a 30.5B parameter Mixture-of-Experts (MoE) model with 128 experts (8 active per forward pass), designed for advanced code generation, repository-scale understanding, and agentic tool use. Built on the Qwen3 architecture, it supports a native context length of 256K tokens (extendable to 1M with Yarn) and performs strongly in tasks involving function calls, browser use, and structured code completion.\\n\\nThis model is optimized for instruction-following without “thinking mode”, and integrates well with OpenAI-compatible tool-use formats. \",\"provider\":{\"@type\":\"Organization\",\"name\":\"Amazon Bedrock\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Qwen\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"amazon bedrock/qwen/qwen3-coder-30b-a3b-instruct--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":0.15},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":0.6}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=amazon/bedrock/qwen/qwen3-coder-30b-a3b-instruct--variant-standard--seq-33\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:22.353Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Anthropic: Claude 3.5 Haiku\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Claude 3.5 Haiku features offers enhanced capabilities in speed, coding accuracy, and tool use. Engineered to excel in real-time applications, it delivers quick response times that are essential for dynamic tasks such as chat interactions and immediate coding suggestions.\\n\\nThis makes it highly suitable for environments that demand both speed and precision, such as software development, customer service bots, and data management systems.\\n\\nThis model is currently pointing to [Claude 3.5 Haiku (2024-10-22)](/anthropic/claude-3-5-haiku-20241022).\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Anthropic\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Anthropic\"},\"softwareVersion\":\"028ec497-a034-40fd-81fe-f51d0a0c640c\",\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":200000},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"anthropic/anthropic/claude-3.5-haiku--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text, image\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":0.8},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":4},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":55.16}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=anthropic/anthropic/claude-3.5-haiku--variant-standard--seq-50\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:22.353Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Anthropic: Claude 3.7 Sonnet\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Claude 3.7 Sonnet is an advanced large language model with improved reasoning, coding, and problem-solving capabilities. It introduces a hybrid reasoning approach, allowing users to choose between rapid responses and extended, step-by-step processing for complex tasks. The model demonstrates notable improvements in coding, particularly in front-end development and full-stack updates, and excels in agentic workflows, where it can autonomously navigate multi-step processes. \\n\\nClaude 3.7 Sonnet maintains performance parity with its predecessor in standard mode while offering an extended reasoning mode for enhanced accuracy in math, coding, and instruction-following tasks.\\n\\nRead more at the [blog post here](https://www.anthropic.com/news/claude-3-7-sonnet)\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Anthropic\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Anthropic\"},\"softwareVersion\":\"30636d20-cda3-4a59-aa0c-1a5b6efba072\",\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":200000},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"anthropic/anthropic/claude-3.7-sonnet--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text, image, file\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":3},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":15},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":51.08}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=anthropic/anthropic/claude-3.7-sonnet--variant-standard--seq-49\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:22.353Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Anthropic: Claude 3.7 Sonnet (thinking)\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Claude 3.7 Sonnet is an advanced large language model with improved reasoning, coding, and problem-solving capabilities. It introduces a hybrid reasoning approach, allowing users to choose between rapid responses and extended, step-by-step processing for complex tasks. The model demonstrates notable improvements in coding, particularly in front-end development and full-stack updates, and excels in agentic workflows, where it can autonomously navigate multi-step processes. \\n\\nClaude 3.7 Sonnet maintains performance parity with its predecessor in standard mode while offering an extended reasoning mode for enhanced accuracy in math, coding, and instruction-following tasks.\\n\\nRead more at the [blog post here](https://www.anthropic.com/news/claude-3-7-sonnet)\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Anthropic\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Anthropic\"},\"softwareVersion\":\"30636d20-cda3-4a59-aa0c-1a5b6efba072\",\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":200000},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"anthropic/anthropic/claude-3.7-sonnet--variant-thinking\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text, image, file\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":3},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":15},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":49.82}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=anthropic/anthropic/claude-3.7-sonnet--variant-thinking--seq-48\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:22.353Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Anthropic: Claude 3 Haiku\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Claude 3 Haiku is Anthropic's fastest and most compact model for\\nnear-instant responsiveness. Quick and accurate targeted performance.\\n\\nSee the launch announcement and benchmark results [here](https://www.anthropic.com/news/claude-3-haiku)\\n\\n#multimodal\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Anthropic\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Anthropic\"},\"softwareVersion\":\"028ec497-a034-40fd-81fe-f51d0a0c640c\",\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":200000},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"anthropic/anthropic/claude-3-haiku--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text, image\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":0.25},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":1.25},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":149.76}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=anthropic/anthropic/claude-3-haiku--variant-standard--seq-51\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:22.353Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Anthropic: Claude 3 Opus\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Claude 3 Opus is Anthropic's most powerful model for highly complex tasks. It boasts top-level performance, intelligence, fluency, and understanding.\\n\\nSee the launch announcement and benchmark results [here](https://www.anthropic.com/news/claude-3-family)\\n\\n#multimodal\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Anthropic\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Anthropic\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":200000},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"anthropic/anthropic/claude-3-opus--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"text, image\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":15},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":75},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":30.6}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=anthropic/anthropic/claude-3-opus--variant-standard--seq-52\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:22.353Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Anthropic: Claude Haiku 4.5\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Claude Haiku 4.5 is Anthropic’s fastest and most efficient model, delivering near-frontier intelligence at a fraction of the cost and latency of larger Claude models. Matching Claude Sonnet 4’s performance across reasoning, coding, and computer-use tasks, Haiku 4.5 brings frontier-level capability to real-time and high-volume applications.\\n\\nIt introduces extended thinking to the Haiku line; enabling controllable reasoning depth, summarized or interleaved thought output, and tool-assisted workflows with full support for coding, bash, web search, and computer-use tools. Scoring \u003e73% on SWE-bench Verified, Haiku 4.5 ranks among the world’s best coding models while maintaining exceptional responsiveness for sub-agents, parallelized execution, and scaled deployment.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Anthropic\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Anthropic\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":200000},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"anthropic/anthropic/claude-haiku-4.5--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"image, text\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":1},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":5},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":103.58}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=anthropic/anthropic/claude-haiku-4.5--variant-standard--seq-43\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:22.353Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Anthropic: Claude Opus 4\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Claude Opus 4 is benchmarked as the world’s best coding model, at time of release, bringing sustained performance on complex, long-running tasks and agent workflows. It sets new benchmarks in software engineering, achieving leading results on SWE-bench (72.5%) and Terminal-bench (43.2%). Opus 4 supports extended, agentic workflows, handling thousands of task steps continuously for hours without degradation. \\n\\nRead more at the [blog post here](https://www.anthropic.com/news/claude-4)\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Anthropic\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Anthropic\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":200000},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"anthropic/anthropic/claude-opus-4--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"image, text, file\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":15},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":75},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":49.65}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=anthropic/anthropic/claude-opus-4--variant-standard--seq-46\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:22.353Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Anthropic: Claude Opus 4.1\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Claude Opus 4.1 is an updated version of Anthropic’s flagship model, offering improved performance in coding, reasoning, and agentic tasks. It achieves 74.5% on SWE-bench Verified and shows notable gains in multi-file code refactoring, debugging precision, and detail-oriented reasoning. The model supports extended thinking up to 64K tokens and is optimized for tasks involving research, data analysis, and tool-assisted reasoning.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Anthropic\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Anthropic\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":200000},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"anthropic/anthropic/claude-opus-4.1--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"image, text, file\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":15},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":75},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":29.8}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=anthropic/anthropic/claude-opus-4.1--variant-standard--seq-45\"}},{\"@type\":\"DataFeedItem\",\"dateModified\":\"2025-11-28T07:00:22.353Z\",\"item\":{\"@type\":\"SoftwareApplication\",\"name\":\"Anthropic: Claude Opus 4.5\",\"applicationCategory\":\"AI language model\",\"operatingSystem\":\"Cloud\",\"description\":\"Claude Opus 4.5 is Anthropic’s frontier reasoning model optimized for complex software engineering, agentic workflows, and long-horizon computer use. It offers strong multimodal capabilities, competitive performance across real-world coding and reasoning benchmarks, and improved robustness to prompt injection. The model is designed to operate efficiently across varied effort levels, enabling developers to trade off speed, depth, and token usage depending on task requirements. It comes with a new parameter to control token efficiency, which can be accessed using the OpenRouter Verbosity parameter with low, medium, or high.\\n\\nOpus 4.5 supports advanced tool use, extended context management, and coordinated multi-agent setups, making it well-suited for autonomous research, debugging, multi-step planning, and spreadsheet/browser manipulation. It delivers substantial gains in structured reasoning, execution reliability, and alignment compared to prior Opus generations, while reducing token overhead and improving performance on long-running tasks.\",\"provider\":{\"@type\":\"Organization\",\"name\":\"Anthropic\"},\"author\":{\"@type\":\"Organization\",\"name\":\"Anthropic\"},\"additionalProperty\":[{\"@type\":\"PropertyValue\",\"name\":\"Context Length\",\"value\":200000},{\"@type\":\"PropertyValue\",\"name\":\"Model ID\",\"value\":\"anthropic/anthropic/claude-opus-4.5--variant-standard\"},{\"@type\":\"PropertyValue\",\"name\":\"Input Modalities\",\"value\":\"file, image, text\"},{\"@type\":\"PropertyValue\",\"name\":\"Output Modalities\",\"value\":\"text\"},{\"@type\":\"PropertyValue\",\"name\":\"Prompt Price (USD / 1M tokens)\",\"value\":5},{\"@type\":\"PropertyValue\",\"name\":\"Output Price (USD / 1M tokens)\",\"value\":25},{\"@type\":\"PropertyValue\",\"name\":\"Throughput (tok/s)\",\"value\":50.04}],\"url\":\"https://codex-mu-eight.vercel.app/llms?uuid=anthropic/anthropic/claude-opus-4.5--variant-standard--seq-42\"}}]}"])</script><script>self.__next_f.push([1,"b:[\"$\",\"$L13\",null,{\"state\":{\"mutations\":[],\"queries\":[{\"state\":{\"data\":{\"pages\":[{\"data\":[{\"id\":\"ai21/ai21/jamba-large-1.7--variant-standard--seq-2\",\"slug\":\"ai21/ai21/jamba-large-1.7--variant-standard\",\"provider\":\"AI21\",\"name\":\"AI21: Jamba Large 1.7\",\"shortName\":\"Jamba Large 1.7\",\"author\":\"AI21\",\"description\":\"Jamba Large 1.7 is the latest model in the Jamba open family, offering improvements in grounding, instruction-following, and overall efficiency. Built on a hybrid SSM-Transformer architecture with a 256K context window, it delivers more accurate, contextually grounded responses and better steerability than previous versions.\",\"modelVersionGroupId\":null,\"contextLength\":256000,\"inputModalities\":[\"text\"],\"outputModalities\":[\"text\"],\"permaslug\":\"ai21/jamba-large-1.7\",\"endpointId\":\"d68054f9-503c-4742-bbbb-32c415d27aa2\",\"promptPrice\":0.000002,\"completionPrice\":0.000008,\"modalityScore\":1,\"throughput\":34.275,\"maxCompletionTokens\":4096,\"supportedParameters\":[\"response_format\",\"max_tokens\",\"temperature\",\"top_p\",\"stop\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:21.583Z\"},{\"id\":\"ai21/ai21/jamba-mini-1.7--variant-standard--seq-1\",\"slug\":\"ai21/ai21/jamba-mini-1.7--variant-standard\",\"provider\":\"AI21\",\"name\":\"AI21: Jamba Mini 1.7\",\"shortName\":\"Jamba Mini 1.7\",\"author\":\"AI21\",\"description\":\"Jamba Mini 1.7 is a compact and efficient member of the Jamba open model family, incorporating key improvements in grounding and instruction-following while maintaining the benefits of the SSM-Transformer hybrid architecture and 256K context window. Despite its compact size, it delivers accurate, contextually grounded responses and improved steerability.\",\"modelVersionGroupId\":null,\"contextLength\":256000,\"inputModalities\":[\"text\"],\"outputModalities\":[\"text\"],\"permaslug\":\"ai21/jamba-mini-1.7\",\"endpointId\":\"5c2327ef-2f21-4a94-845b-7f5673d6bdac\",\"promptPrice\":2e-7,\"completionPrice\":4e-7,\"modalityScore\":1,\"throughput\":168.269,\"maxCompletionTokens\":4096,\"supportedParameters\":[\"response_format\",\"max_tokens\",\"temperature\",\"top_p\",\"stop\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:21.583Z\"},{\"id\":\"aionlabs/aion-labs/aion-1.0--variant-standard--seq-3\",\"slug\":\"aionlabs/aion-labs/aion-1.0--variant-standard\",\"provider\":\"AionLabs\",\"name\":\"AionLabs: Aion-1.0\",\"shortName\":\"Aion-1.0\",\"author\":\"AionLabs\",\"description\":\"Aion-1.0 is a multi-model system designed for high performance across various tasks, including reasoning and coding. It is built on DeepSeek-R1, augmented with additional models and techniques such as Tree of Thoughts (ToT) and Mixture of Experts (MoE). It is Aion Lab's most powerful reasoning model.\",\"modelVersionGroupId\":null,\"contextLength\":131072,\"inputModalities\":[\"text\"],\"outputModalities\":[\"text\"],\"permaslug\":\"aion-labs/aion-1.0\",\"endpointId\":\"68716168-5661-4ed3-a841-caa166d7913e\",\"promptPrice\":0.000004,\"completionPrice\":0.000008,\"modalityScore\":1,\"throughput\":61.074,\"maxCompletionTokens\":32768,\"supportedParameters\":[\"reasoning\",\"include_reasoning\",\"max_tokens\",\"temperature\",\"top_p\"],\"scrapedAt\":\"2025-11-28T07:00:21.780Z\"},{\"id\":\"aionlabs/aion-labs/aion-1.0-mini--variant-standard--seq-4\",\"slug\":\"aionlabs/aion-labs/aion-1.0-mini--variant-standard\",\"provider\":\"AionLabs\",\"name\":\"AionLabs: Aion-1.0-Mini\",\"shortName\":\"Aion-1.0-Mini\",\"author\":\"AionLabs\",\"description\":\"Aion-1.0-Mini 32B parameter model is a distilled version of the DeepSeek-R1 model, designed for strong performance in reasoning domains such as mathematics, coding, and logic. It is a modified variant of a FuseAI model that outperforms R1-Distill-Qwen-32B and R1-Distill-Llama-70B, with benchmark results available on its [Hugging Face page](https://huggingface.co/FuseAI/FuseO1-DeepSeekR1-QwQ-SkyT1-32B-Preview), independently replicated for verification.\",\"modelVersionGroupId\":null,\"contextLength\":131072,\"inputModalities\":[\"text\"],\"outputModalities\":[\"text\"],\"permaslug\":\"aion-labs/aion-1.0-mini\",\"endpointId\":\"83eeb9eb-edea-49f5-abad-3f245f46420f\",\"promptPrice\":7e-7,\"completionPrice\":0.0000014,\"modalityScore\":1,\"throughput\":184.575,\"maxCompletionTokens\":32768,\"supportedParameters\":[\"reasoning\",\"include_reasoning\",\"max_tokens\",\"temperature\",\"top_p\"],\"scrapedAt\":\"2025-11-28T07:00:21.780Z\"},{\"id\":\"aionlabs/aion-labs/aion-rp-llama-3.1-8b--variant-standard--seq-5\",\"slug\":\"aionlabs/aion-labs/aion-rp-llama-3.1-8b--variant-standard\",\"provider\":\"AionLabs\",\"name\":\"AionLabs: Aion-RP 1.0 (8B)\",\"shortName\":\"Aion-RP 1.0 (8B)\",\"author\":\"AionLabs\",\"description\":\"Aion-RP-Llama-3.1-8B ranks the highest in the character evaluation portion of the RPBench-Auto benchmark, a roleplaying-specific variant of Arena-Hard-Auto, where LLMs evaluate each other’s responses. It is a fine-tuned base model rather than an instruct model, designed to produce more natural and varied writing.\",\"modelVersionGroupId\":null,\"contextLength\":32768,\"inputModalities\":[\"text\"],\"outputModalities\":[\"text\"],\"permaslug\":\"aion-labs/aion-rp-llama-3.1-8b\",\"endpointId\":\"fd456f92-a506-4249-8527-988e9c1d84b3\",\"promptPrice\":2e-7,\"completionPrice\":2e-7,\"modalityScore\":1,\"throughput\":44.57,\"maxCompletionTokens\":32768,\"supportedParameters\":[\"max_tokens\",\"temperature\",\"top_p\"],\"scrapedAt\":\"2025-11-28T07:00:21.780Z\"},{\"id\":\"alibaba/qwen/qwen3-235b-a22b-2507--variant-standard--seq-20\",\"slug\":\"alibaba/qwen/qwen3-235b-a22b-2507--variant-standard\",\"provider\":\"Alibaba\",\"name\":\"Qwen: Qwen3 235B A22B Instruct 2507\",\"shortName\":\"Qwen3 235B A22B Instruct 2507\",\"author\":\"Qwen\",\"description\":\"Qwen3-235B-A22B-Instruct-2507 is a multilingual, instruction-tuned mixture-of-experts language model based on the Qwen3-235B architecture, with 22B active parameters per forward pass. It is optimized for general-purpose text generation, including instruction following, logical reasoning, math, code, and tool usage. The model supports a native 262K context length and does not implement \\\"thinking mode\\\" (\u003cthink\u003e blocks).\\n\\nCompared to its base variant, this version delivers significant gains in knowledge coverage, long-context reasoning, coding benchmarks, and alignment with open-ended tasks. It is particularly strong on multilingual understanding, math reasoning (e.g., AIME, HMMT), and alignment evaluations like Arena-Hard and WritingBench.\",\"modelVersionGroupId\":null,\"contextLength\":131072,\"inputModalities\":[\"text\"],\"outputModalities\":[\"text\"],\"permaslug\":\"qwen/qwen3-235b-a22b-07-25\",\"endpointId\":\"813ce28a-1334-47e3-881e-a7f8833b34b4\",\"promptPrice\":2.3e-7,\"completionPrice\":9.2e-7,\"modalityScore\":1,\"throughput\":85.526,\"maxCompletionTokens\":32768,\"supportedParameters\":[\"response_format\",\"max_tokens\",\"temperature\",\"top_p\",\"seed\",\"presence_penalty\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:21.983Z\"},{\"id\":\"alibaba/qwen/qwen3-30b-a3b-instruct-2507--variant-standard--seq-18\",\"slug\":\"alibaba/qwen/qwen3-30b-a3b-instruct-2507--variant-standard\",\"provider\":\"Alibaba\",\"name\":\"Qwen: Qwen3 30B A3B Instruct 2507\",\"shortName\":\"Qwen3 30B A3B Instruct 2507\",\"author\":\"Qwen\",\"description\":\"Qwen3-30B-A3B-Instruct-2507 is a 30.5B-parameter mixture-of-experts language model from Qwen, with 3.3B active parameters per inference. It operates in non-thinking mode and is designed for high-quality instruction following, multilingual understanding, and agentic tool use. Post-trained on instruction data, it demonstrates competitive performance across reasoning (AIME, ZebraLogic), coding (MultiPL-E, LiveCodeBench), and alignment (IFEval, WritingBench) benchmarks. It outperforms its non-instruct variant on subjective and open-ended tasks while retaining strong factual and coding performance.\",\"modelVersionGroupId\":null,\"contextLength\":131072,\"inputModalities\":[\"text\"],\"outputModalities\":[\"text\"],\"permaslug\":\"qwen/qwen3-30b-a3b-instruct-2507\",\"endpointId\":\"64103172-a7d0-4c84-a908-fdd09043dca2\",\"promptPrice\":2e-7,\"completionPrice\":8e-7,\"modalityScore\":1,\"throughput\":79.566,\"maxCompletionTokens\":32768,\"supportedParameters\":[\"max_tokens\",\"temperature\",\"top_p\",\"seed\",\"presence_penalty\",\"response_format\"],\"scrapedAt\":\"2025-11-28T07:00:21.983Z\"},{\"id\":\"alibaba/qwen/qwen3-30b-a3b-thinking-2507--variant-standard--seq-16\",\"slug\":\"alibaba/qwen/qwen3-30b-a3b-thinking-2507--variant-standard\",\"provider\":\"Alibaba\",\"name\":\"Qwen: Qwen3 30B A3B Thinking 2507\",\"shortName\":\"Qwen3 30B A3B Thinking 2507\",\"author\":\"Qwen\",\"description\":\"Qwen3-30B-A3B-Thinking-2507 is a 30B parameter Mixture-of-Experts reasoning model optimized for complex tasks requiring extended multi-step thinking. The model is designed specifically for “thinking mode,” where internal reasoning traces are separated from final answers.\\n\\nCompared to earlier Qwen3-30B releases, this version improves performance across logical reasoning, mathematics, science, coding, and multilingual benchmarks. It also demonstrates stronger instruction following, tool use, and alignment with human preferences. With higher reasoning efficiency and extended output budgets, it is best suited for advanced research, competitive problem solving, and agentic applications requiring structured long-context reasoning.\",\"modelVersionGroupId\":null,\"contextLength\":131072,\"inputModalities\":[\"text\"],\"outputModalities\":[\"text\"],\"permaslug\":\"qwen/qwen3-30b-a3b-thinking-2507\",\"endpointId\":\"86834360-63c5-482a-8ef9-85b22ddf0980\",\"promptPrice\":2e-7,\"completionPrice\":0.0000024,\"modalityScore\":1,\"throughput\":155.2335,\"maxCompletionTokens\":32768,\"supportedParameters\":[\"reasoning\",\"include_reasoning\",\"max_tokens\",\"temperature\",\"top_p\",\"seed\",\"presence_penalty\",\"response_format\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:21.983Z\"},{\"id\":\"alibaba/qwen/qwen3-coder-30b-a3b-instruct--variant-standard--seq-17\",\"slug\":\"alibaba/qwen/qwen3-coder-30b-a3b-instruct--variant-standard\",\"provider\":\"Alibaba\",\"name\":\"Qwen: Qwen3 Coder 30B A3B Instruct\",\"shortName\":\"Qwen3 Coder 30B A3B Instruct\",\"author\":\"Qwen\",\"description\":\"Qwen3-Coder-30B-A3B-Instruct is a 30.5B parameter Mixture-of-Experts (MoE) model with 128 experts (8 active per forward pass), designed for advanced code generation, repository-scale understanding, and agentic tool use. Built on the Qwen3 architecture, it supports a native context length of 256K tokens (extendable to 1M with Yarn) and performs strongly in tasks involving function calls, browser use, and structured code completion.\\n\\nThis model is optimized for instruction-following without “thinking mode”, and integrates well with OpenAI-compatible tool-use formats. \",\"modelVersionGroupId\":null,\"contextLength\":200000,\"inputModalities\":[\"text\"],\"outputModalities\":[\"text\"],\"permaslug\":\"qwen/qwen3-coder-30b-a3b-instruct\",\"endpointId\":\"4977811c-1f48-4d0f-8b67-5acabe42ab7c\",\"promptPrice\":7.5e-7,\"completionPrice\":0.00000375,\"modalityScore\":1,\"throughput\":107.142,\"maxCompletionTokens\":65536,\"supportedParameters\":[\"max_tokens\",\"temperature\",\"top_p\",\"seed\",\"presence_penalty\",\"response_format\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:21.983Z\"},{\"id\":\"alibaba/qwen/qwen3-coder--variant-standard--seq-19\",\"slug\":\"alibaba/qwen/qwen3-coder--variant-standard\",\"provider\":\"Alibaba\",\"name\":\"Qwen: Qwen3 Coder 480B A35B\",\"shortName\":\"Qwen3 Coder 480B A35B\",\"author\":\"Qwen\",\"description\":\"Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) code generation model developed by the Qwen team. It is optimized for agentic coding tasks such as function calling, tool use, and long-context reasoning over repositories. The model features 480 billion total parameters, with 35 billion active per forward pass (8 out of 160 experts).\\n\\nPricing for the Alibaba endpoints varies by context length. Once a request is greater than 128k input tokens, the higher pricing is used.\",\"modelVersionGroupId\":null,\"contextLength\":262144,\"inputModalities\":[\"text\"],\"outputModalities\":[\"text\"],\"permaslug\":\"qwen/qwen3-coder-480b-a35b-07-25\",\"endpointId\":\"eb365617-e7c9-41fa-98ea-a3aa2183ce58\",\"promptPrice\":0.0000015,\"completionPrice\":0.0000075,\"modalityScore\":1,\"throughput\":70.08,\"maxCompletionTokens\":65536,\"supportedParameters\":[\"structured_outputs\",\"response_format\",\"max_tokens\",\"temperature\",\"top_p\",\"seed\",\"presence_penalty\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:21.983Z\"},{\"id\":\"alibaba/qwen/qwen3-coder-flash--variant-standard--seq-11\",\"slug\":\"alibaba/qwen/qwen3-coder-flash--variant-standard\",\"provider\":\"Alibaba\",\"name\":\"Qwen: Qwen3 Coder Flash\",\"shortName\":\"Qwen3 Coder Flash\",\"author\":\"Qwen\",\"description\":\"Qwen3 Coder Flash is Alibaba's fast and cost efficient version of their proprietary Qwen3 Coder Plus. It is a powerful coding agent model specializing in autonomous programming via tool calling and environment interaction, combining coding proficiency with versatile general-purpose abilities.\",\"modelVersionGroupId\":null,\"contextLength\":128000,\"inputModalities\":[\"text\"],\"outputModalities\":[\"text\"],\"permaslug\":\"qwen/qwen3-coder-flash\",\"endpointId\":\"ded5f1ed-4249-476c-a19e-df63b005a8b7\",\"promptPrice\":3e-7,\"completionPrice\":0.0000015,\"modalityScore\":1,\"throughput\":107.4735,\"maxCompletionTokens\":65536,\"supportedParameters\":[\"max_tokens\",\"temperature\",\"top_p\",\"seed\",\"presence_penalty\",\"response_format\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:21.983Z\"},{\"id\":\"alibaba/qwen/qwen3-coder-plus--variant-standard--seq-10\",\"slug\":\"alibaba/qwen/qwen3-coder-plus--variant-standard\",\"provider\":\"Alibaba\",\"name\":\"Qwen: Qwen3 Coder Plus\",\"shortName\":\"Qwen3 Coder Plus\",\"author\":\"Qwen\",\"description\":\"Qwen3 Coder Plus is Alibaba's proprietary version of the Open Source Qwen3 Coder 480B A35B. It is a powerful coding agent model specializing in autonomous programming via tool calling and environment interaction, combining coding proficiency with versatile general-purpose abilities.\",\"modelVersionGroupId\":null,\"contextLength\":128000,\"inputModalities\":[\"text\"],\"outputModalities\":[\"text\"],\"permaslug\":\"qwen/qwen3-coder-plus\",\"endpointId\":\"bc011219-e28e-416d-9cca-9aaf022f227e\",\"promptPrice\":0.000001,\"completionPrice\":0.000005,\"modalityScore\":1,\"throughput\":65.656,\"maxCompletionTokens\":65536,\"supportedParameters\":[\"structured_outputs\",\"response_format\",\"max_tokens\",\"temperature\",\"top_p\",\"seed\",\"presence_penalty\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:21.983Z\"},{\"id\":\"alibaba/qwen/qwen3-max--variant-standard--seq-9\",\"slug\":\"alibaba/qwen/qwen3-max--variant-standard\",\"provider\":\"Alibaba\",\"name\":\"Qwen: Qwen3 Max\",\"shortName\":\"Qwen3 Max\",\"author\":\"Qwen\",\"description\":\"Qwen3-Max is an updated release built on the Qwen3 series, offering major improvements in reasoning, instruction following, multilingual support, and long-tail knowledge coverage compared to the January 2025 version. It delivers higher accuracy in math, coding, logic, and science tasks, follows complex instructions in Chinese and English more reliably, reduces hallucinations, and produces higher-quality responses for open-ended Q\u0026A, writing, and conversation. The model supports over 100 languages with stronger translation and commonsense reasoning, and is optimized for retrieval-augmented generation (RAG) and tool calling, though it does not include a dedicated “thinking” mode.\",\"modelVersionGroupId\":null,\"contextLength\":256000,\"inputModalities\":[\"text\"],\"outputModalities\":[\"text\"],\"permaslug\":\"qwen/qwen3-max\",\"endpointId\":\"5e31ba3c-d5bf-4ced-a52d-96e2e6635f95\",\"promptPrice\":0.0000012,\"completionPrice\":0.000006,\"modalityScore\":1,\"throughput\":34.36,\"maxCompletionTokens\":32768,\"supportedParameters\":[\"response_format\",\"max_tokens\",\"temperature\",\"top_p\",\"seed\",\"presence_penalty\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:21.983Z\"},{\"id\":\"alibaba/qwen/qwen3-next-80b-a3b-instruct--variant-standard--seq-13\",\"slug\":\"alibaba/qwen/qwen3-next-80b-a3b-instruct--variant-standard\",\"provider\":\"Alibaba\",\"name\":\"Qwen: Qwen3 Next 80B A3B Instruct\",\"shortName\":\"Qwen3 Next 80B A3B Instruct\",\"author\":\"Qwen\",\"description\":\"$14\",\"modelVersionGroupId\":null,\"contextLength\":131072,\"inputModalities\":[\"text\"],\"outputModalities\":[\"text\"],\"permaslug\":\"qwen/qwen3-next-80b-a3b-instruct-2509\",\"endpointId\":\"fb6f5134-ae6f-4ffa-9d64-16c2731bef93\",\"promptPrice\":1.5e-7,\"completionPrice\":0.0000012,\"modalityScore\":1,\"throughput\":114.285,\"maxCompletionTokens\":32768,\"supportedParameters\":[\"max_tokens\",\"temperature\",\"top_p\",\"seed\",\"presence_penalty\",\"response_format\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:21.983Z\"},{\"id\":\"alibaba/qwen/qwen3-next-80b-a3b-thinking--variant-standard--seq-12\",\"slug\":\"alibaba/qwen/qwen3-next-80b-a3b-thinking--variant-standard\",\"provider\":\"Alibaba\",\"name\":\"Qwen: Qwen3 Next 80B A3B Thinking\",\"shortName\":\"Qwen3 Next 80B A3B Thinking\",\"author\":\"Qwen\",\"description\":\"Qwen3-Next-80B-A3B-Thinking is a reasoning-first chat model in the Qwen3-Next line that outputs structured “thinking” traces by default. It’s designed for hard multi-step problems; math proofs, code synthesis/debugging, logic, and agentic planning, and reports strong results across knowledge, reasoning, coding, alignment, and multilingual evaluations. Compared with prior Qwen3 variants, it emphasizes stability under long chains of thought and efficient scaling during inference, and it is tuned to follow complex instructions while reducing repetitive or off-task behavior.\\n\\nThe model is suitable for agent frameworks and tool use (function calling), retrieval-heavy workflows, and standardized benchmarking where step-by-step solutions are required. It supports long, detailed completions and leverages throughput-oriented techniques (e.g., multi-token prediction) for faster generation. Note that it operates in thinking-only mode.\",\"modelVersionGroupId\":null,\"contextLength\":262144,\"inputModalities\":[\"text\"],\"outputModalities\":[\"text\"],\"permaslug\":\"qwen/qwen3-next-80b-a3b-thinking-2509\",\"endpointId\":\"1b075e5f-051b-4d60-9bc3-20f69ff6355d\",\"promptPrice\":5e-7,\"completionPrice\":0.000006,\"modalityScore\":1,\"throughput\":151.4595,\"maxCompletionTokens\":32768,\"supportedParameters\":[\"reasoning\",\"include_reasoning\",\"max_tokens\",\"temperature\",\"top_p\",\"seed\",\"presence_penalty\",\"response_format\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:21.983Z\"},{\"id\":\"alibaba/qwen/qwen3-vl-235b-a22b-instruct--variant-standard--seq-8\",\"slug\":\"alibaba/qwen/qwen3-vl-235b-a22b-instruct--variant-standard\",\"provider\":\"Alibaba\",\"name\":\"Qwen: Qwen3 VL 235B A22B Instruct\",\"shortName\":\"Qwen3 VL 235B A22B Instruct\",\"author\":\"Qwen\",\"description\":\"$15\",\"modelVersionGroupId\":null,\"contextLength\":131072,\"inputModalities\":[\"text\",\"image\"],\"outputModalities\":[\"text\"],\"permaslug\":\"qwen/qwen3-vl-235b-a22b-instruct\",\"endpointId\":\"ffa79fb8-8758-4c19-a790-40706e9033d6\",\"promptPrice\":4e-7,\"completionPrice\":0.0000016,\"modalityScore\":2,\"throughput\":42.126,\"maxCompletionTokens\":32768,\"supportedParameters\":[\"structured_outputs\",\"response_format\",\"max_tokens\",\"temperature\",\"top_p\",\"seed\",\"presence_penalty\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:21.983Z\"},{\"id\":\"alibaba/qwen/qwen3-vl-8b-instruct--variant-standard--seq-7\",\"slug\":\"alibaba/qwen/qwen3-vl-8b-instruct--variant-standard\",\"provider\":\"Alibaba\",\"name\":\"Qwen: Qwen3 VL 8B Instruct\",\"shortName\":\"Qwen3 VL 8B Instruct\",\"author\":\"Qwen\",\"description\":\"Qwen3-VL-8B-Instruct is a multimodal vision-language model from the Qwen3-VL series, built for high-fidelity understanding and reasoning across text, images, and video. It features improved multimodal fusion with Interleaved-MRoPE for long-horizon temporal reasoning, DeepStack for fine-grained visual-text alignment, and text-timestamp alignment for precise event localization.\\n\\nThe model supports a native 256K-token context window, extensible to 1M tokens, and handles both static and dynamic media inputs for tasks like document parsing, visual question answering, spatial reasoning, and GUI control. It achieves text understanding comparable to leading LLMs while expanding OCR coverage to 32 languages and enhancing robustness under varied visual conditions.\",\"modelVersionGroupId\":null,\"contextLength\":256000,\"inputModalities\":[\"image\",\"text\"],\"outputModalities\":[\"text\"],\"permaslug\":\"qwen/qwen3-vl-8b-instruct\",\"endpointId\":\"0122dcb7-c876-4445-8a8e-610a7b44d12b\",\"promptPrice\":1.8e-7,\"completionPrice\":7e-7,\"modalityScore\":2,\"throughput\":121.779,\"maxCompletionTokens\":32768,\"supportedParameters\":[\"structured_outputs\",\"response_format\",\"max_tokens\",\"temperature\",\"top_p\",\"seed\",\"presence_penalty\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:21.983Z\"},{\"id\":\"alibaba/qwen/qwen3-vl-8b-thinking--variant-standard--seq-6\",\"slug\":\"alibaba/qwen/qwen3-vl-8b-thinking--variant-standard\",\"provider\":\"Alibaba\",\"name\":\"Qwen: Qwen3 VL 8B Thinking\",\"shortName\":\"Qwen3 VL 8B Thinking\",\"author\":\"Qwen\",\"description\":\"Qwen3-VL-8B-Thinking is the reasoning-optimized variant of the Qwen3-VL-8B multimodal model, designed for advanced visual and textual reasoning across complex scenes, documents, and temporal sequences. It integrates enhanced multimodal alignment and long-context processing (native 256K, expandable to 1M tokens) for tasks such as scientific visual analysis, causal inference, and mathematical reasoning over image or video inputs.\\n\\nCompared to the Instruct edition, the Thinking version introduces deeper visual-language fusion and deliberate reasoning pathways that improve performance on long-chain logic tasks, STEM problem-solving, and multi-step video understanding. It achieves stronger temporal grounding via Interleaved-MRoPE and timestamp-aware embeddings, while maintaining robust OCR, multilingual comprehension, and text generation on par with large text-only LLMs.\",\"modelVersionGroupId\":null,\"contextLength\":256000,\"inputModalities\":[\"image\",\"text\"],\"outputModalities\":[\"text\"],\"permaslug\":\"qwen/qwen3-vl-8b-thinking\",\"endpointId\":\"6cb46014-347b-4d67-aefa-7b83e065b8c3\",\"promptPrice\":1.8e-7,\"completionPrice\":0.0000021,\"modalityScore\":2,\"throughput\":65.7325,\"maxCompletionTokens\":32768,\"supportedParameters\":[\"reasoning\",\"include_reasoning\",\"structured_outputs\",\"response_format\",\"max_tokens\",\"temperature\",\"top_p\",\"seed\",\"presence_penalty\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:21.983Z\"},{\"id\":\"alibaba/qwen/qwen-max--variant-standard--seq-25\",\"slug\":\"alibaba/qwen/qwen-max--variant-standard\",\"provider\":\"Alibaba\",\"name\":\"Qwen: Qwen-Max \",\"shortName\":\"Qwen-Max\",\"author\":\"Qwen\",\"description\":\"Qwen-Max, based on Qwen2.5, provides the best inference performance among [Qwen models](/qwen), especially for complex multi-step tasks. It's a large-scale MoE model that has been pretrained on over 20 trillion tokens and further post-trained with curated Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) methodologies. The parameter count is unknown.\",\"modelVersionGroupId\":null,\"contextLength\":32768,\"inputModalities\":[\"text\"],\"outputModalities\":[\"text\"],\"permaslug\":\"qwen/qwen-max-2025-01-25\",\"endpointId\":\"8cf90481-400f-473d-b188-1bed01001a01\",\"promptPrice\":0.0000016,\"completionPrice\":0.0000064,\"modalityScore\":1,\"throughput\":24.376,\"maxCompletionTokens\":8192,\"supportedParameters\":[\"max_tokens\",\"temperature\",\"top_p\",\"seed\",\"presence_penalty\",\"response_format\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:21.983Z\"},{\"id\":\"alibaba/qwen/qwen-plus--variant-standard--seq-24\",\"slug\":\"alibaba/qwen/qwen-plus--variant-standard\",\"provider\":\"Alibaba\",\"name\":\"Qwen: Qwen-Plus\",\"shortName\":\"Qwen-Plus\",\"author\":\"Qwen\",\"description\":\"Qwen-Plus, based on the Qwen2.5 foundation model, is a 131K context model with a balanced performance, speed, and cost combination.\",\"modelVersionGroupId\":null,\"contextLength\":131072,\"inputModalities\":[\"text\"],\"outputModalities\":[\"text\"],\"permaslug\":\"qwen/qwen-plus-2025-01-25\",\"endpointId\":\"47e88ae3-94ed-4d6a-96bb-a150dd354853\",\"promptPrice\":4e-7,\"completionPrice\":0.0000012,\"modalityScore\":1,\"throughput\":63.3365,\"maxCompletionTokens\":8192,\"supportedParameters\":[\"max_tokens\",\"temperature\",\"top_p\",\"seed\",\"presence_penalty\",\"response_format\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:21.983Z\"},{\"id\":\"alibaba/qwen/qwen-plus-2025-07-28--variant-standard--seq-14\",\"slug\":\"alibaba/qwen/qwen-plus-2025-07-28--variant-standard\",\"provider\":\"Alibaba\",\"name\":\"Qwen: Qwen Plus 0728\",\"shortName\":\"Qwen Plus 0728\",\"author\":\"Qwen\",\"description\":\"Qwen Plus 0728, based on the Qwen3 foundation model, is a 1 million context hybrid reasoning model with a balanced performance, speed, and cost combination.\",\"modelVersionGroupId\":null,\"contextLength\":1000000,\"inputModalities\":[\"text\"],\"outputModalities\":[\"text\"],\"permaslug\":\"qwen/qwen-plus-2025-07-28\",\"endpointId\":\"23a86833-b536-4529-94c4-a18746e95c60\",\"promptPrice\":4e-7,\"completionPrice\":0.0000012,\"modalityScore\":1,\"throughput\":64.4885,\"maxCompletionTokens\":32768,\"supportedParameters\":[\"structured_outputs\",\"response_format\",\"max_tokens\",\"temperature\",\"top_p\",\"seed\",\"presence_penalty\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:21.983Z\"},{\"id\":\"alibaba/qwen/qwen-plus-2025-07-28--variant-thinking--seq-15\",\"slug\":\"alibaba/qwen/qwen-plus-2025-07-28--variant-thinking\",\"provider\":\"Alibaba\",\"name\":\"Qwen: Qwen Plus 0728 (thinking)\",\"shortName\":\"Qwen Plus 0728 Thinking\",\"author\":\"Qwen\",\"description\":\"Qwen Plus 0728, based on the Qwen3 foundation model, is a 1 million context hybrid reasoning model with a balanced performance, speed, and cost combination.\",\"modelVersionGroupId\":null,\"contextLength\":1000000,\"inputModalities\":[\"text\"],\"outputModalities\":[\"text\"],\"permaslug\":\"qwen/qwen-plus-2025-07-28\",\"endpointId\":\"3d871d0b-eade-4fd4-95b3-1d5f25e2b0c3\",\"promptPrice\":4e-7,\"completionPrice\":0.000004,\"modalityScore\":1,\"throughput\":81.151,\"maxCompletionTokens\":32768,\"supportedParameters\":[\"reasoning\",\"include_reasoning\",\"structured_outputs\",\"response_format\",\"max_tokens\",\"temperature\",\"top_p\",\"seed\",\"presence_penalty\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:21.983Z\"},{\"id\":\"alibaba/qwen/qwen-turbo--variant-standard--seq-23\",\"slug\":\"alibaba/qwen/qwen-turbo--variant-standard\",\"provider\":\"Alibaba\",\"name\":\"Qwen: Qwen-Turbo\",\"shortName\":\"Qwen-Turbo\",\"author\":\"Qwen\",\"description\":\"Qwen-Turbo, based on Qwen2.5, is a 1M context model that provides fast speed and low cost, suitable for simple tasks.\",\"modelVersionGroupId\":null,\"contextLength\":1000000,\"inputModalities\":[\"text\"],\"outputModalities\":[\"text\"],\"permaslug\":\"qwen/qwen-turbo-2024-11-01\",\"endpointId\":\"5a7f2177-7886-4059-a0c5-a43af75e1302\",\"promptPrice\":5e-8,\"completionPrice\":2e-7,\"modalityScore\":1,\"throughput\":58.995,\"maxCompletionTokens\":8192,\"supportedParameters\":[\"max_tokens\",\"temperature\",\"top_p\",\"seed\",\"presence_penalty\",\"response_format\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:21.983Z\"},{\"id\":\"alibaba/qwen/qwen-vl-max--variant-standard--seq-22\",\"slug\":\"alibaba/qwen/qwen-vl-max--variant-standard\",\"provider\":\"Alibaba\",\"name\":\"Qwen: Qwen VL Max\",\"shortName\":\"Qwen VL Max\",\"author\":\"Qwen\",\"description\":\"Qwen VL Max is a visual understanding model with 7500 tokens context length. It excels in delivering optimal performance for a broader spectrum of complex tasks.\\n\",\"modelVersionGroupId\":null,\"contextLength\":131072,\"inputModalities\":[\"text\",\"image\"],\"outputModalities\":[\"text\"],\"permaslug\":\"qwen/qwen-vl-max-2025-01-25\",\"endpointId\":\"40a33c72-3801-49f3-ac2a-966d0b249981\",\"promptPrice\":8e-7,\"completionPrice\":0.0000032,\"modalityScore\":2,\"throughput\":50.059,\"maxCompletionTokens\":8192,\"supportedParameters\":[\"max_tokens\",\"temperature\",\"top_p\",\"seed\",\"presence_penalty\",\"response_format\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:21.983Z\"},{\"id\":\"alibaba/qwen/qwen-vl-plus--variant-standard--seq-21\",\"slug\":\"alibaba/qwen/qwen-vl-plus--variant-standard\",\"provider\":\"Alibaba\",\"name\":\"Qwen: Qwen VL Plus\",\"shortName\":\"Qwen VL Plus\",\"author\":\"Qwen\",\"description\":\"Qwen's Enhanced Large Visual Language Model. Significantly upgraded for detailed recognition capabilities and text recognition abilities, supporting ultra-high pixel resolutions up to millions of pixels and extreme aspect ratios for image input. It delivers significant performance across a broad range of visual tasks.\\n\",\"modelVersionGroupId\":null,\"contextLength\":7500,\"inputModalities\":[\"text\",\"image\"],\"outputModalities\":[\"text\"],\"permaslug\":\"qwen/qwen-vl-plus\",\"endpointId\":\"df9c15ac-870b-40e5-aa43-e4b3b44951f7\",\"promptPrice\":2.1e-7,\"completionPrice\":6.3e-7,\"modalityScore\":2,\"throughput\":102.779,\"maxCompletionTokens\":1500,\"supportedParameters\":[\"max_tokens\",\"temperature\",\"top_p\",\"seed\",\"presence_penalty\",\"response_format\"],\"scrapedAt\":\"2025-11-28T07:00:21.983Z\"},{\"id\":\"amazon bedrock/anthropic/claude-3.5-haiku--variant-standard--seq-40\",\"slug\":\"amazon bedrock/anthropic/claude-3.5-haiku--variant-standard\",\"provider\":\"Amazon Bedrock\",\"name\":\"Anthropic: Claude 3.5 Haiku\",\"shortName\":\"Claude 3.5 Haiku\",\"author\":\"Anthropic\",\"description\":\"Claude 3.5 Haiku features offers enhanced capabilities in speed, coding accuracy, and tool use. Engineered to excel in real-time applications, it delivers quick response times that are essential for dynamic tasks such as chat interactions and immediate coding suggestions.\\n\\nThis makes it highly suitable for environments that demand both speed and precision, such as software development, customer service bots, and data management systems.\\n\\nThis model is currently pointing to [Claude 3.5 Haiku (2024-10-22)](/anthropic/claude-3-5-haiku-20241022).\",\"modelVersionGroupId\":\"028ec497-a034-40fd-81fe-f51d0a0c640c\",\"contextLength\":200000,\"inputModalities\":[\"text\",\"image\"],\"outputModalities\":[\"text\"],\"permaslug\":\"anthropic/claude-3-5-haiku\",\"endpointId\":\"a4e00256-c60a-4c4a-82a2-8c9a34de0711\",\"promptPrice\":8e-7,\"completionPrice\":0.000004,\"modalityScore\":2,\"throughput\":67.166,\"maxCompletionTokens\":8192,\"supportedParameters\":[\"max_tokens\",\"temperature\",\"top_p\",\"top_k\",\"stop\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:22.173Z\"},{\"id\":\"amazon bedrock/anthropic/claude-3.5-sonnet--variant-standard--seq-41\",\"slug\":\"amazon bedrock/anthropic/claude-3.5-sonnet--variant-standard\",\"provider\":\"Amazon Bedrock\",\"name\":\"Anthropic: Claude 3.5 Sonnet\",\"shortName\":\"Claude 3.5 Sonnet\",\"author\":\"Anthropic\",\"description\":\"New Claude 3.5 Sonnet delivers better-than-Opus capabilities, faster-than-Sonnet speeds, at the same Sonnet prices. Sonnet is particularly good at:\\n\\n- Coding: Scores ~49% on SWE-Bench Verified, higher than the last best score, and without any fancy prompt scaffolding\\n- Data science: Augments human data science expertise; navigates unstructured data while using multiple tools for insights\\n- Visual processing: excelling at interpreting charts, graphs, and images, accurately transcribing text to derive insights beyond just the text alone\\n- Agentic tasks: exceptional tool use, making it great at agentic tasks (i.e. complex, multi-step problem solving tasks that require engaging with other systems)\\n\\n#multimodal\",\"modelVersionGroupId\":\"30636d20-cda3-4a59-aa0c-1a5b6efba072\",\"contextLength\":200000,\"inputModalities\":[\"text\",\"image\",\"file\"],\"outputModalities\":[\"text\"],\"permaslug\":\"anthropic/claude-3.5-sonnet\",\"endpointId\":\"0e1957b3-205b-49d5-8413-13af3f68676d\",\"promptPrice\":0.000003,\"completionPrice\":0.000015,\"modalityScore\":3,\"throughput\":47.07,\"maxCompletionTokens\":8192,\"supportedParameters\":[\"max_tokens\",\"temperature\",\"top_p\",\"top_k\",\"stop\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:22.173Z\"},{\"id\":\"amazon bedrock/anthropic/claude-3.7-sonnet--variant-standard--seq-36\",\"slug\":\"amazon bedrock/anthropic/claude-3.7-sonnet--variant-standard\",\"provider\":\"Amazon Bedrock\",\"name\":\"Anthropic: Claude 3.7 Sonnet\",\"shortName\":\"Claude 3.7 Sonnet\",\"author\":\"Anthropic\",\"description\":\"Claude 3.7 Sonnet is an advanced large language model with improved reasoning, coding, and problem-solving capabilities. It introduces a hybrid reasoning approach, allowing users to choose between rapid responses and extended, step-by-step processing for complex tasks. The model demonstrates notable improvements in coding, particularly in front-end development and full-stack updates, and excels in agentic workflows, where it can autonomously navigate multi-step processes. \\n\\nClaude 3.7 Sonnet maintains performance parity with its predecessor in standard mode while offering an extended reasoning mode for enhanced accuracy in math, coding, and instruction-following tasks.\\n\\nRead more at the [blog post here](https://www.anthropic.com/news/claude-3-7-sonnet)\",\"modelVersionGroupId\":\"30636d20-cda3-4a59-aa0c-1a5b6efba072\",\"contextLength\":200000,\"inputModalities\":[\"text\",\"image\",\"file\"],\"outputModalities\":[\"text\"],\"permaslug\":\"anthropic/claude-3-7-sonnet-20250219\",\"endpointId\":\"ca3453dc-9de2-4ddc-aac3-0aa1f30e54c4\",\"promptPrice\":0.000003,\"completionPrice\":0.000015,\"modalityScore\":3,\"throughput\":104.638,\"maxCompletionTokens\":128000,\"supportedParameters\":[\"reasoning\",\"include_reasoning\",\"max_tokens\",\"temperature\",\"top_p\",\"top_k\",\"stop\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:22.173Z\"},{\"id\":\"amazon bedrock/anthropic/claude-haiku-4.5--variant-standard--seq-28\",\"slug\":\"amazon bedrock/anthropic/claude-haiku-4.5--variant-standard\",\"provider\":\"Amazon Bedrock\",\"name\":\"Anthropic: Claude Haiku 4.5\",\"shortName\":\"Claude Haiku 4.5\",\"author\":\"Anthropic\",\"description\":\"Claude Haiku 4.5 is Anthropic’s fastest and most efficient model, delivering near-frontier intelligence at a fraction of the cost and latency of larger Claude models. Matching Claude Sonnet 4’s performance across reasoning, coding, and computer-use tasks, Haiku 4.5 brings frontier-level capability to real-time and high-volume applications.\\n\\nIt introduces extended thinking to the Haiku line; enabling controllable reasoning depth, summarized or interleaved thought output, and tool-assisted workflows with full support for coding, bash, web search, and computer-use tools. Scoring \u003e73% on SWE-bench Verified, Haiku 4.5 ranks among the world’s best coding models while maintaining exceptional responsiveness for sub-agents, parallelized execution, and scaled deployment.\",\"modelVersionGroupId\":null,\"contextLength\":200000,\"inputModalities\":[\"image\",\"text\"],\"outputModalities\":[\"text\"],\"permaslug\":\"anthropic/claude-4.5-haiku-20251001\",\"endpointId\":\"d000261c-5aeb-47d1-ac87-ad64dbe1cdb6\",\"promptPrice\":0.000001,\"completionPrice\":0.000005,\"modalityScore\":2,\"throughput\":121.3735,\"maxCompletionTokens\":64000,\"supportedParameters\":[\"reasoning\",\"include_reasoning\",\"max_tokens\",\"temperature\",\"top_p\",\"top_k\",\"stop\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:22.173Z\"},{\"id\":\"amazon bedrock/anthropic/claude-opus-4--variant-standard--seq-34\",\"slug\":\"amazon bedrock/anthropic/claude-opus-4--variant-standard\",\"provider\":\"Amazon Bedrock\",\"name\":\"Anthropic: Claude Opus 4\",\"shortName\":\"Claude Opus 4\",\"author\":\"Anthropic\",\"description\":\"Claude Opus 4 is benchmarked as the world’s best coding model, at time of release, bringing sustained performance on complex, long-running tasks and agent workflows. It sets new benchmarks in software engineering, achieving leading results on SWE-bench (72.5%) and Terminal-bench (43.2%). Opus 4 supports extended, agentic workflows, handling thousands of task steps continuously for hours without degradation. \\n\\nRead more at the [blog post here](https://www.anthropic.com/news/claude-4)\",\"modelVersionGroupId\":null,\"contextLength\":200000,\"inputModalities\":[\"image\",\"text\",\"file\"],\"outputModalities\":[\"text\"],\"permaslug\":\"anthropic/claude-4-opus-20250522\",\"endpointId\":\"a1d11e23-4e05-42cf-9fa3-6cacdb8d384a\",\"promptPrice\":0.000015,\"completionPrice\":0.000075,\"modalityScore\":3,\"throughput\":20.22,\"maxCompletionTokens\":32000,\"supportedParameters\":[\"reasoning\",\"include_reasoning\",\"max_tokens\",\"temperature\",\"top_p\",\"top_k\",\"stop\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:22.173Z\"},{\"id\":\"amazon bedrock/anthropic/claude-opus-4.1--variant-standard--seq-32\",\"slug\":\"amazon bedrock/anthropic/claude-opus-4.1--variant-standard\",\"provider\":\"Amazon Bedrock\",\"name\":\"Anthropic: Claude Opus 4.1\",\"shortName\":\"Claude Opus 4.1\",\"author\":\"Anthropic\",\"description\":\"Claude Opus 4.1 is an updated version of Anthropic’s flagship model, offering improved performance in coding, reasoning, and agentic tasks. It achieves 74.5% on SWE-bench Verified and shows notable gains in multi-file code refactoring, debugging precision, and detail-oriented reasoning. The model supports extended thinking up to 64K tokens and is optimized for tasks involving research, data analysis, and tool-assisted reasoning.\",\"modelVersionGroupId\":null,\"contextLength\":200000,\"inputModalities\":[\"image\",\"text\",\"file\"],\"outputModalities\":[\"text\"],\"permaslug\":\"anthropic/claude-4.1-opus-20250805\",\"endpointId\":\"ece5c3c6-e437-4297-92fc-150a1771ec56\",\"promptPrice\":0.000015,\"completionPrice\":0.000075,\"modalityScore\":3,\"throughput\":17.6955,\"maxCompletionTokens\":null,\"supportedParameters\":[\"reasoning\",\"include_reasoning\",\"max_tokens\",\"temperature\",\"top_p\",\"top_k\",\"stop\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:22.173Z\"},{\"id\":\"amazon bedrock/anthropic/claude-opus-4.5--variant-standard--seq-26\",\"slug\":\"amazon bedrock/anthropic/claude-opus-4.5--variant-standard\",\"provider\":\"Amazon Bedrock\",\"name\":\"Anthropic: Claude Opus 4.5\",\"shortName\":\"Claude Opus 4.5\",\"author\":\"Anthropic\",\"description\":\"$16\",\"modelVersionGroupId\":null,\"contextLength\":200000,\"inputModalities\":[\"file\",\"image\",\"text\"],\"outputModalities\":[\"text\"],\"permaslug\":\"anthropic/claude-4.5-opus-20251124\",\"endpointId\":\"de32581c-2e11-4f54-8d29-e8a6e19d3038\",\"promptPrice\":0.000005,\"completionPrice\":0.000025,\"modalityScore\":3,\"throughput\":63.5635,\"maxCompletionTokens\":32000,\"supportedParameters\":[\"reasoning\",\"include_reasoning\",\"max_tokens\",\"temperature\",\"top_k\",\"stop\",\"tool_choice\",\"tools\",\"verbosity\"],\"scrapedAt\":\"2025-11-28T07:00:22.173Z\"},{\"id\":\"amazon bedrock/anthropic/claude-sonnet-4--variant-standard--seq-35\",\"slug\":\"amazon bedrock/anthropic/claude-sonnet-4--variant-standard\",\"provider\":\"Amazon Bedrock\",\"name\":\"Anthropic: Claude Sonnet 4\",\"shortName\":\"Claude Sonnet 4\",\"author\":\"Anthropic\",\"description\":\"Claude Sonnet 4 significantly enhances the capabilities of its predecessor, Sonnet 3.7, excelling in both coding and reasoning tasks with improved precision and controllability. Achieving state-of-the-art performance on SWE-bench (72.7%), Sonnet 4 balances capability and computational efficiency, making it suitable for a broad range of applications from routine coding tasks to complex software development projects. Key enhancements include improved autonomous codebase navigation, reduced error rates in agent-driven workflows, and increased reliability in following intricate instructions. Sonnet 4 is optimized for practical everyday use, providing advanced reasoning capabilities while maintaining efficiency and responsiveness in diverse internal and external scenarios.\\n\\nRead more at the [blog post here](https://www.anthropic.com/news/claude-4)\",\"modelVersionGroupId\":null,\"contextLength\":200000,\"inputModalities\":[\"image\",\"text\",\"file\"],\"outputModalities\":[\"text\"],\"permaslug\":\"anthropic/claude-4-sonnet-20250522\",\"endpointId\":\"b2abae2d-f225-48ce-8672-cb1d08da5d7d\",\"promptPrice\":0.000003,\"completionPrice\":0.000015,\"modalityScore\":3,\"throughput\":101.351,\"maxCompletionTokens\":64000,\"supportedParameters\":[\"reasoning\",\"include_reasoning\",\"max_tokens\",\"temperature\",\"top_p\",\"top_k\",\"stop\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:22.173Z\"},{\"id\":\"amazon bedrock/anthropic/claude-sonnet-4.5--variant-standard--seq-29\",\"slug\":\"amazon bedrock/anthropic/claude-sonnet-4.5--variant-standard\",\"provider\":\"Amazon Bedrock\",\"name\":\"Anthropic: Claude Sonnet 4.5\",\"shortName\":\"Claude Sonnet 4.5\",\"author\":\"Anthropic\",\"description\":\"Claude Sonnet 4.5 is Anthropic’s most advanced Sonnet model to date, optimized for real-world agents and coding workflows. It delivers state-of-the-art performance on coding benchmarks such as SWE-bench Verified, with improvements across system design, code security, and specification adherence. The model is designed for extended autonomous operation, maintaining task continuity across sessions and providing fact-based progress tracking.\\n\\nSonnet 4.5 also introduces stronger agentic capabilities, including improved tool orchestration, speculative parallel execution, and more efficient context and memory management. With enhanced context tracking and awareness of token usage across tool calls, it is particularly well-suited for multi-context and long-running workflows. Use cases span software engineering, cybersecurity, financial analysis, research agents, and other domains requiring sustained reasoning and tool use.\",\"modelVersionGroupId\":null,\"contextLength\":1000000,\"inputModalities\":[\"text\",\"image\",\"file\"],\"outputModalities\":[\"text\"],\"permaslug\":\"anthropic/claude-4.5-sonnet-20250929\",\"endpointId\":\"d8bee848-f020-454a-b2be-4a7ce868f99f\",\"promptPrice\":0.000003,\"completionPrice\":0.000015,\"modalityScore\":3,\"throughput\":66.706,\"maxCompletionTokens\":64000,\"supportedParameters\":[\"reasoning\",\"include_reasoning\",\"max_tokens\",\"temperature\",\"top_p\",\"top_k\",\"stop\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:22.173Z\"},{\"id\":\"amazon bedrock/openai/gpt-oss-120b--variant-standard--seq-30\",\"slug\":\"amazon bedrock/openai/gpt-oss-120b--variant-standard\",\"provider\":\"Amazon Bedrock\",\"name\":\"OpenAI: gpt-oss-120b\",\"shortName\":\"gpt-oss-120b\",\"author\":\"OpenAI\",\"description\":\"gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation.\",\"modelVersionGroupId\":null,\"contextLength\":131072,\"inputModalities\":[\"text\"],\"outputModalities\":[\"text\"],\"permaslug\":\"openai/gpt-oss-120b\",\"endpointId\":\"b3f80c05-e31a-44f6-b63f-414467b87553\",\"promptPrice\":1.5e-7,\"completionPrice\":6e-7,\"modalityScore\":1,\"throughput\":852.39,\"maxCompletionTokens\":null,\"supportedParameters\":[\"reasoning\",\"include_reasoning\",\"max_tokens\",\"temperature\",\"top_p\",\"top_k\",\"stop\"],\"scrapedAt\":\"2025-11-28T07:00:22.173Z\"},{\"id\":\"amazon bedrock/openai/gpt-oss-20b--variant-standard--seq-31\",\"slug\":\"amazon bedrock/openai/gpt-oss-20b--variant-standard\",\"provider\":\"Amazon Bedrock\",\"name\":\"OpenAI: gpt-oss-20b\",\"shortName\":\"gpt-oss-20b\",\"author\":\"OpenAI\",\"description\":\"gpt-oss-20b is an open-weight 21B parameter model released by OpenAI under the Apache 2.0 license. It uses a Mixture-of-Experts (MoE) architecture with 3.6B active parameters per forward pass, optimized for lower-latency inference and deployability on consumer or single-GPU hardware. The model is trained in OpenAI’s Harmony response format and supports reasoning level configuration, fine-tuning, and agentic capabilities including function calling, tool use, and structured outputs.\",\"modelVersionGroupId\":null,\"contextLength\":131072,\"inputModalities\":[\"text\"],\"outputModalities\":[\"text\"],\"permaslug\":\"openai/gpt-oss-20b\",\"endpointId\":\"3dab0c5c-47bb-4f84-aaf0-dcc5aec260bf\",\"promptPrice\":7e-8,\"completionPrice\":1.5e-7,\"modalityScore\":1,\"throughput\":1611.111,\"maxCompletionTokens\":null,\"supportedParameters\":[\"reasoning\",\"include_reasoning\",\"max_tokens\",\"temperature\",\"top_p\",\"top_k\",\"stop\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:22.173Z\"},{\"id\":\"amazon bedrock/amazon/nova-lite-v1--variant-standard--seq-37\",\"slug\":\"amazon bedrock/amazon/nova-lite-v1--variant-standard\",\"provider\":\"Amazon Bedrock\",\"name\":\"Amazon: Nova Lite 1.0\",\"shortName\":\"Nova Lite 1.0\",\"author\":\"Amazon\",\"description\":\"Amazon Nova Lite 1.0 is a very low-cost multimodal model from Amazon that focused on fast processing of image, video, and text inputs to generate text output. Amazon Nova Lite can handle real-time customer interactions, document analysis, and visual question-answering tasks with high accuracy.\\n\\nWith an input context of 300K tokens, it can analyze multiple images or up to 30 minutes of video in a single input.\",\"modelVersionGroupId\":null,\"contextLength\":300000,\"inputModalities\":[\"text\",\"image\"],\"outputModalities\":[\"text\"],\"permaslug\":\"amazon/nova-lite-v1\",\"endpointId\":\"72eda073-d180-4482-8e4f-81051cb66f7e\",\"promptPrice\":6e-8,\"completionPrice\":2.4e-7,\"modalityScore\":2,\"throughput\":130.3255,\"maxCompletionTokens\":5120,\"supportedParameters\":[\"max_tokens\",\"temperature\",\"top_p\",\"top_k\",\"stop\",\"tools\"],\"scrapedAt\":\"2025-11-28T07:00:22.173Z\"},{\"id\":\"amazon bedrock/amazon/nova-micro-v1--variant-standard--seq-38\",\"slug\":\"amazon bedrock/amazon/nova-micro-v1--variant-standard\",\"provider\":\"Amazon Bedrock\",\"name\":\"Amazon: Nova Micro 1.0\",\"shortName\":\"Nova Micro 1.0\",\"author\":\"Amazon\",\"description\":\"Amazon Nova Micro 1.0 is a text-only model that delivers the lowest latency responses in the Amazon Nova family of models at a very low cost. With a context length of 128K tokens and optimized for speed and cost, Amazon Nova Micro excels at tasks such as text summarization, translation, content classification, interactive chat, and brainstorming. It has  simple mathematical reasoning and coding abilities.\",\"modelVersionGroupId\":null,\"contextLength\":128000,\"inputModalities\":[\"text\"],\"outputModalities\":[\"text\"],\"permaslug\":\"amazon/nova-micro-v1\",\"endpointId\":\"474f0074-66f9-42f0-a866-81a2ffebb001\",\"promptPrice\":3.5e-8,\"completionPrice\":1.4e-7,\"modalityScore\":1,\"throughput\":361.128,\"maxCompletionTokens\":5120,\"supportedParameters\":[\"max_tokens\",\"temperature\",\"top_p\",\"top_k\",\"stop\",\"tools\"],\"scrapedAt\":\"2025-11-28T07:00:22.173Z\"},{\"id\":\"amazon bedrock/amazon/nova-premier-v1--variant-standard--seq-27\",\"slug\":\"amazon bedrock/amazon/nova-premier-v1--variant-standard\",\"provider\":\"Amazon Bedrock\",\"name\":\"Amazon: Nova Premier 1.0\",\"shortName\":\"Nova Premier 1.0\",\"author\":\"Amazon\",\"description\":\"Amazon Nova Premier is the most capable of Amazon’s multimodal models for complex reasoning tasks and for use as the best teacher for distilling custom models.\",\"modelVersionGroupId\":null,\"contextLength\":1000000,\"inputModalities\":[\"text\",\"image\"],\"outputModalities\":[\"text\"],\"permaslug\":\"amazon/nova-premier-v1\",\"endpointId\":\"6e4da481-6c8d-45d0-a3f5-11a9ba527485\",\"promptPrice\":0.0000025,\"completionPrice\":0.0000125,\"modalityScore\":2,\"throughput\":39.8975,\"maxCompletionTokens\":32000,\"supportedParameters\":[\"max_tokens\",\"temperature\",\"top_p\",\"top_k\",\"stop\",\"tools\"],\"scrapedAt\":\"2025-11-28T07:00:22.173Z\"},{\"id\":\"amazon bedrock/amazon/nova-pro-v1--variant-standard--seq-39\",\"slug\":\"amazon bedrock/amazon/nova-pro-v1--variant-standard\",\"provider\":\"Amazon Bedrock\",\"name\":\"Amazon: Nova Pro 1.0\",\"shortName\":\"Nova Pro 1.0\",\"author\":\"Amazon\",\"description\":\"Amazon Nova Pro 1.0 is a capable multimodal model from Amazon focused on providing a combination of accuracy, speed, and cost for a wide range of tasks. As of December 2024, it achieves state-of-the-art performance on key benchmarks including visual question answering (TextVQA) and video understanding (VATEX).\\n\\nAmazon Nova Pro demonstrates strong capabilities in processing both visual and textual information and at analyzing financial documents.\\n\\n**NOTE**: Video input is not supported at this time.\",\"modelVersionGroupId\":null,\"contextLength\":300000,\"inputModalities\":[\"text\",\"image\"],\"outputModalities\":[\"text\"],\"permaslug\":\"amazon/nova-pro-v1\",\"endpointId\":\"959381a4-8054-450f-9daf-5fcab64ba9aa\",\"promptPrice\":8e-7,\"completionPrice\":0.0000032,\"modalityScore\":2,\"throughput\":92.817,\"maxCompletionTokens\":5120,\"supportedParameters\":[\"max_tokens\",\"temperature\",\"top_p\",\"top_k\",\"stop\",\"tools\"],\"scrapedAt\":\"2025-11-28T07:00:22.173Z\"},{\"id\":\"amazon bedrock/qwen/qwen3-coder-30b-a3b-instruct--variant-standard--seq-33\",\"slug\":\"amazon bedrock/qwen/qwen3-coder-30b-a3b-instruct--variant-standard\",\"provider\":\"Amazon Bedrock\",\"name\":\"Qwen: Qwen3 Coder 30B A3B Instruct\",\"shortName\":\"Qwen3 Coder 30B A3B Instruct\",\"author\":\"Qwen\",\"description\":\"Qwen3-Coder-30B-A3B-Instruct is a 30.5B parameter Mixture-of-Experts (MoE) model with 128 experts (8 active per forward pass), designed for advanced code generation, repository-scale understanding, and agentic tool use. Built on the Qwen3 architecture, it supports a native context length of 256K tokens (extendable to 1M with Yarn) and performs strongly in tasks involving function calls, browser use, and structured code completion.\\n\\nThis model is optimized for instruction-following without “thinking mode”, and integrates well with OpenAI-compatible tool-use formats. \",\"modelVersionGroupId\":null,\"contextLength\":null,\"inputModalities\":[\"text\"],\"outputModalities\":[\"text\"],\"permaslug\":\"qwen/qwen3-coder-30b-a3b-instruct\",\"endpointId\":\"e127a93b-3750-4e1f-8743-d1023dad1dc4\",\"promptPrice\":1.5e-7,\"completionPrice\":6e-7,\"modalityScore\":1,\"throughput\":null,\"maxCompletionTokens\":null,\"supportedParameters\":[\"max_tokens\",\"temperature\",\"top_p\",\"top_k\",\"stop\",\"tool_choice\",\"tools\"],\"scrapedAt\":\"2025-11-28T07:00:22.173Z\"},{\"id\":\"anthropic/anthropic/claude-3.5-haiku--variant-standard--seq-50\",\"slug\":\"anthropic/anthropic/claude-3.5-haiku--variant-standard\",\"provider\":\"Anthropic\",\"name\":\"Anthropic: Claude 3.5 Haiku\",\"shortName\":\"Claude 3.5 Haiku\",\"author\":\"Anthropic\",\"description\":\"Claude 3.5 Haiku features offers enhanced capabilities in speed, coding accuracy, and tool use. Engineered to excel in real-time applications, it delivers quick response times that are essential for dynamic tasks such as chat interactions and immediate coding suggestions.\\n\\nThis makes it highly suitable for environments that demand both speed and precision, such as software development, customer service bots, and data management systems.\\n\\nThis model is currently pointing to [Claude 3.5 Haiku (2024-10-22)](/anthropic/claude-3-5-haiku-20241022).\",\"modelVersionGroupId\":\"028ec497-a034-40fd-81fe-f51d0a0c640c\",\"contextLength\":200000,\"inputModalities\":[\"text\",\"image\"],\"outputModalities\":[\"text\"],\"permaslug\":\"anthropic/claude-3-5-haiku\",\"endpointId\":\"ced809d4-9715-4634-8b4d-ea0e09d6bb85\",\"promptPrice\":8e-7,\"completionPrice\":0.000004,\"modalityScore\":2,\"throughput\":55.1605,\"maxCompletionTokens\":8192,\"supportedParameters\":[\"max_tokens\",\"temperature\",\"top_p\",\"top_k\",\"stop\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:22.353Z\"},{\"id\":\"anthropic/anthropic/claude-3.7-sonnet--variant-standard--seq-49\",\"slug\":\"anthropic/anthropic/claude-3.7-sonnet--variant-standard\",\"provider\":\"Anthropic\",\"name\":\"Anthropic: Claude 3.7 Sonnet\",\"shortName\":\"Claude 3.7 Sonnet\",\"author\":\"Anthropic\",\"description\":\"Claude 3.7 Sonnet is an advanced large language model with improved reasoning, coding, and problem-solving capabilities. It introduces a hybrid reasoning approach, allowing users to choose between rapid responses and extended, step-by-step processing for complex tasks. The model demonstrates notable improvements in coding, particularly in front-end development and full-stack updates, and excels in agentic workflows, where it can autonomously navigate multi-step processes. \\n\\nClaude 3.7 Sonnet maintains performance parity with its predecessor in standard mode while offering an extended reasoning mode for enhanced accuracy in math, coding, and instruction-following tasks.\\n\\nRead more at the [blog post here](https://www.anthropic.com/news/claude-3-7-sonnet)\",\"modelVersionGroupId\":\"30636d20-cda3-4a59-aa0c-1a5b6efba072\",\"contextLength\":200000,\"inputModalities\":[\"text\",\"image\",\"file\"],\"outputModalities\":[\"text\"],\"permaslug\":\"anthropic/claude-3-7-sonnet-20250219\",\"endpointId\":\"1347c8dc-12f5-47df-8355-3ec7e80a8c67\",\"promptPrice\":0.000003,\"completionPrice\":0.000015,\"modalityScore\":3,\"throughput\":51.079,\"maxCompletionTokens\":128000,\"supportedParameters\":[\"max_tokens\",\"top_p\",\"temperature\",\"stop\",\"reasoning\",\"include_reasoning\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:22.353Z\"},{\"id\":\"anthropic/anthropic/claude-3.7-sonnet--variant-thinking--seq-48\",\"slug\":\"anthropic/anthropic/claude-3.7-sonnet--variant-thinking\",\"provider\":\"Anthropic\",\"name\":\"Anthropic: Claude 3.7 Sonnet (thinking)\",\"shortName\":\"Claude 3.7 Sonnet Thinking\",\"author\":\"Anthropic\",\"description\":\"Claude 3.7 Sonnet is an advanced large language model with improved reasoning, coding, and problem-solving capabilities. It introduces a hybrid reasoning approach, allowing users to choose between rapid responses and extended, step-by-step processing for complex tasks. The model demonstrates notable improvements in coding, particularly in front-end development and full-stack updates, and excels in agentic workflows, where it can autonomously navigate multi-step processes. \\n\\nClaude 3.7 Sonnet maintains performance parity with its predecessor in standard mode while offering an extended reasoning mode for enhanced accuracy in math, coding, and instruction-following tasks.\\n\\nRead more at the [blog post here](https://www.anthropic.com/news/claude-3-7-sonnet)\",\"modelVersionGroupId\":\"30636d20-cda3-4a59-aa0c-1a5b6efba072\",\"contextLength\":200000,\"inputModalities\":[\"text\",\"image\",\"file\"],\"outputModalities\":[\"text\"],\"permaslug\":\"anthropic/claude-3-7-sonnet-20250219\",\"endpointId\":\"0223918d-eb42-4c84-9ed2-a9a27f287f24\",\"promptPrice\":0.000003,\"completionPrice\":0.000015,\"modalityScore\":3,\"throughput\":49.824,\"maxCompletionTokens\":128000,\"supportedParameters\":[\"max_tokens\",\"top_p\",\"temperature\",\"stop\",\"reasoning\",\"include_reasoning\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:22.353Z\"},{\"id\":\"anthropic/anthropic/claude-3-haiku--variant-standard--seq-51\",\"slug\":\"anthropic/anthropic/claude-3-haiku--variant-standard\",\"provider\":\"Anthropic\",\"name\":\"Anthropic: Claude 3 Haiku\",\"shortName\":\"Claude 3 Haiku\",\"author\":\"Anthropic\",\"description\":\"Claude 3 Haiku is Anthropic's fastest and most compact model for\\nnear-instant responsiveness. Quick and accurate targeted performance.\\n\\nSee the launch announcement and benchmark results [here](https://www.anthropic.com/news/claude-3-haiku)\\n\\n#multimodal\",\"modelVersionGroupId\":\"028ec497-a034-40fd-81fe-f51d0a0c640c\",\"contextLength\":200000,\"inputModalities\":[\"text\",\"image\"],\"outputModalities\":[\"text\"],\"permaslug\":\"anthropic/claude-3-haiku\",\"endpointId\":\"8661a1db-b0cf-4eb2-ba04-c2a79f698682\",\"promptPrice\":2.5e-7,\"completionPrice\":0.00000125,\"modalityScore\":2,\"throughput\":149.7555,\"maxCompletionTokens\":4096,\"supportedParameters\":[\"max_tokens\",\"temperature\",\"top_p\",\"top_k\",\"stop\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:22.353Z\"},{\"id\":\"anthropic/anthropic/claude-3-opus--variant-standard--seq-52\",\"slug\":\"anthropic/anthropic/claude-3-opus--variant-standard\",\"provider\":\"Anthropic\",\"name\":\"Anthropic: Claude 3 Opus\",\"shortName\":\"Claude 3 Opus\",\"author\":\"Anthropic\",\"description\":\"Claude 3 Opus is Anthropic's most powerful model for highly complex tasks. It boasts top-level performance, intelligence, fluency, and understanding.\\n\\nSee the launch announcement and benchmark results [here](https://www.anthropic.com/news/claude-3-family)\\n\\n#multimodal\",\"modelVersionGroupId\":null,\"contextLength\":200000,\"inputModalities\":[\"text\",\"image\"],\"outputModalities\":[\"text\"],\"permaslug\":\"anthropic/claude-3-opus\",\"endpointId\":\"ee74a4e0-2863-4f51-99a5-997c31c48ae7\",\"promptPrice\":0.000015,\"completionPrice\":0.000075,\"modalityScore\":2,\"throughput\":30.598,\"maxCompletionTokens\":4096,\"supportedParameters\":[\"max_tokens\",\"temperature\",\"top_p\",\"top_k\",\"stop\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:22.353Z\"},{\"id\":\"anthropic/anthropic/claude-haiku-4.5--variant-standard--seq-43\",\"slug\":\"anthropic/anthropic/claude-haiku-4.5--variant-standard\",\"provider\":\"Anthropic\",\"name\":\"Anthropic: Claude Haiku 4.5\",\"shortName\":\"Claude Haiku 4.5\",\"author\":\"Anthropic\",\"description\":\"Claude Haiku 4.5 is Anthropic’s fastest and most efficient model, delivering near-frontier intelligence at a fraction of the cost and latency of larger Claude models. Matching Claude Sonnet 4’s performance across reasoning, coding, and computer-use tasks, Haiku 4.5 brings frontier-level capability to real-time and high-volume applications.\\n\\nIt introduces extended thinking to the Haiku line; enabling controllable reasoning depth, summarized or interleaved thought output, and tool-assisted workflows with full support for coding, bash, web search, and computer-use tools. Scoring \u003e73% on SWE-bench Verified, Haiku 4.5 ranks among the world’s best coding models while maintaining exceptional responsiveness for sub-agents, parallelized execution, and scaled deployment.\",\"modelVersionGroupId\":null,\"contextLength\":200000,\"inputModalities\":[\"image\",\"text\"],\"outputModalities\":[\"text\"],\"permaslug\":\"anthropic/claude-4.5-haiku-20251001\",\"endpointId\":\"41d2915a-92e6-4993-b537-210b4e10cba8\",\"promptPrice\":0.000001,\"completionPrice\":0.000005,\"modalityScore\":2,\"throughput\":103.575,\"maxCompletionTokens\":64000,\"supportedParameters\":[\"max_tokens\",\"top_p\",\"temperature\",\"stop\",\"reasoning\",\"include_reasoning\",\"tools\",\"tool_choice\",\"top_k\"],\"scrapedAt\":\"2025-11-28T07:00:22.353Z\"},{\"id\":\"anthropic/anthropic/claude-opus-4--variant-standard--seq-46\",\"slug\":\"anthropic/anthropic/claude-opus-4--variant-standard\",\"provider\":\"Anthropic\",\"name\":\"Anthropic: Claude Opus 4\",\"shortName\":\"Claude Opus 4\",\"author\":\"Anthropic\",\"description\":\"Claude Opus 4 is benchmarked as the world’s best coding model, at time of release, bringing sustained performance on complex, long-running tasks and agent workflows. It sets new benchmarks in software engineering, achieving leading results on SWE-bench (72.5%) and Terminal-bench (43.2%). Opus 4 supports extended, agentic workflows, handling thousands of task steps continuously for hours without degradation. \\n\\nRead more at the [blog post here](https://www.anthropic.com/news/claude-4)\",\"modelVersionGroupId\":null,\"contextLength\":200000,\"inputModalities\":[\"image\",\"text\",\"file\"],\"outputModalities\":[\"text\"],\"permaslug\":\"anthropic/claude-4-opus-20250522\",\"endpointId\":\"9ea0cd22-4494-4a94-9199-c83c992bdbe1\",\"promptPrice\":0.000015,\"completionPrice\":0.000075,\"modalityScore\":3,\"throughput\":49.6465,\"maxCompletionTokens\":32000,\"supportedParameters\":[\"max_tokens\",\"top_p\",\"temperature\",\"stop\",\"reasoning\",\"include_reasoning\",\"tools\",\"tool_choice\"],\"scrapedAt\":\"2025-11-28T07:00:22.353Z\"},{\"id\":\"anthropic/anthropic/claude-opus-4.1--variant-standard--seq-45\",\"slug\":\"anthropic/anthropic/claude-opus-4.1--variant-standard\",\"provider\":\"Anthropic\",\"name\":\"Anthropic: Claude Opus 4.1\",\"shortName\":\"Claude Opus 4.1\",\"author\":\"Anthropic\",\"description\":\"Claude Opus 4.1 is an updated version of Anthropic’s flagship model, offering improved performance in coding, reasoning, and agentic tasks. It achieves 74.5% on SWE-bench Verified and shows notable gains in multi-file code refactoring, debugging precision, and detail-oriented reasoning. The model supports extended thinking up to 64K tokens and is optimized for tasks involving research, data analysis, and tool-assisted reasoning.\",\"modelVersionGroupId\":null,\"contextLength\":200000,\"inputModalities\":[\"image\",\"text\",\"file\"],\"outputModalities\":[\"text\"],\"permaslug\":\"anthropic/claude-4.1-opus-20250805\",\"endpointId\":\"ca4e491c-208c-4bd0-b808-35e0ad56bc52\",\"promptPrice\":0.000015,\"completionPrice\":0.000075,\"modalityScore\":3,\"throughput\":29.7995,\"maxCompletionTokens\":32000,\"supportedParameters\":[\"max_tokens\",\"temperature\",\"stop\",\"reasoning\",\"include_reasoning\",\"tools\",\"tool_choice\",\"structured_outputs\",\"response_format\"],\"scrapedAt\":\"2025-11-28T07:00:22.353Z\"},{\"id\":\"anthropic/anthropic/claude-opus-4.5--variant-standard--seq-42\",\"slug\":\"anthropic/anthropic/claude-opus-4.5--variant-standard\",\"provider\":\"Anthropic\",\"name\":\"Anthropic: Claude Opus 4.5\",\"shortName\":\"Claude Opus 4.5\",\"author\":\"Anthropic\",\"description\":\"$17\",\"modelVersionGroupId\":null,\"contextLength\":200000,\"inputModalities\":[\"file\",\"image\",\"text\"],\"outputModalities\":[\"text\"],\"permaslug\":\"anthropic/claude-4.5-opus-20251124\",\"endpointId\":\"be883404-eb42-4b2d-b6e4-c7daa3aa8d62\",\"promptPrice\":0.000005,\"completionPrice\":0.000025,\"modalityScore\":3,\"throughput\":50.0365,\"maxCompletionTokens\":64000,\"supportedParameters\":[\"max_tokens\",\"temperature\",\"stop\",\"reasoning\",\"include_reasoning\",\"tool_choice\",\"tools\",\"structured_outputs\",\"response_format\",\"verbosity\"],\"scrapedAt\":\"2025-11-28T07:00:22.353Z\"}],\"meta\":{\"totalRowCount\":784,\"filterRowCount\":784,\"facets\":{\"provider\":{\"rows\":[{\"value\":\"Azure\",\"total\":19},{\"value\":\"Google Vertex\",\"total\":40},{\"value\":\"Groq\",\"total\":12},{\"value\":\"Together\",\"total\":44},{\"value\":\"Fireworks\",\"total\":17},{\"value\":\"OpenAI\",\"total\":48},{\"value\":\"Anthropic\",\"total\":11},{\"value\":\"Google AI Studio\",\"total\":19},{\"value\":\"Amazon Bedrock\",\"total\":16},{\"value\":\"Mistral\",\"total\":28},{\"value\":\"Cohere\",\"total\":4},{\"value\":\"xAI\",\"total\":8},{\"value\":\"Perplexity\",\"total\":6},{\"value\":\"DeepSeek\",\"total\":1},{\"value\":\"Cerebras\",\"total\":6},{\"value\":\"SambaNova\",\"total\":10},{\"value\":\"DeepInfra\",\"total\":87},{\"value\":\"Cloudflare\",\"total\":14},{\"value\":\"NVIDIA\",\"total\":2},{\"value\":\"Alibaba\",\"total\":20},{\"value\":\"MoonshotAI\",\"total\":4},{\"value\":\"BaseTen\",\"total\":9},{\"value\":\"Nebius\",\"total\":25},{\"value\":\"Crusoe\",\"total\":6},{\"value\":\"Friendli\",\"total\":10},{\"value\":\"Hyperbolic\",\"total\":19},{\"value\":\"MiniMax\",\"total\":3},{\"value\":\"AI21\",\"total\":2},{\"value\":\"SiliconFlow\",\"total\":32},{\"value\":\"Novita\",\"total\":64},{\"value\":\"Inflection\",\"total\":2},{\"value\":\"Venice\",\"total\":8},{\"value\":\"Chutes\",\"total\":55},{\"value\":\"Z.AI\",\"total\":7},{\"value\":\"Weights and Biases\",\"total\":8},{\"value\":\"Phala\",\"total\":6},{\"value\":\"AtlasCloud\",\"total\":22},{\"value\":\"Parasail\",\"total\":27},{\"value\":\"NCompass\",\"total\":6},{\"value\":\"Inception\",\"total\":2},{\"value\":\"Relace\",\"total\":1},{\"value\":\"Morph\",\"total\":2},{\"value\":\"Infermatic\",\"total\":3},{\"value\":\"AionLabs\",\"total\":3},{\"value\":\"Mancer\",\"total\":9},{\"value\":\"NextBit\",\"total\":19},{\"value\":\"Liquid\",\"total\":2},{\"value\":\"OpenInference\",\"total\":2},{\"value\":\"GMICloud\",\"total\":9},{\"value\":\"Switchpoint\",\"total\":1},{\"value\":\"Featherless\",\"total\":2},{\"value\":\"Avian\",\"total\":1},{\"value\":\"Stealth\",\"total\":1}],\"total\":784},\"author\":{\"rows\":[{\"value\":\"OpenAI\",\"total\":102},{\"value\":\"Anthropic\",\"total\":32},{\"value\":\"xAI\",\"total\":8},{\"value\":\"Google\",\"total\":50},{\"value\":\"Meta\",\"total\":76},{\"value\":\"Perplexity\",\"total\":6},{\"value\":\"DeepSeek\",\"total\":71},{\"value\":\"Z.AI\",\"total\":34},{\"value\":\"Qwen\",\"total\":165},{\"value\":\"Mistral\",\"total\":53},{\"value\":\"Cohere\",\"total\":4},{\"value\":\"MoonshotAI\",\"total\":33},{\"value\":\"NVIDIA\",\"total\":8},{\"value\":\"Microsoft\",\"total\":10},{\"value\":\"Amazon\",\"total\":4},{\"value\":\"Alibaba\",\"total\":3},{\"value\":\"Baidu\",\"total\":6},{\"value\":\"Tencent\",\"total\":1},{\"value\":\"ByteDance\",\"total\":1},{\"value\":\"AI21\",\"total\":2},{\"value\":\"Inflection\",\"total\":2},{\"value\":\"Meituan\",\"total\":2},{\"value\":\"AionLabs\",\"total\":3},{\"value\":\"Arcee AI\",\"total\":4},{\"value\":\"AllenAI\",\"total\":3},{\"value\":\"ArliAI\",\"total\":1},{\"value\":\"Liquid\",\"total\":2},{\"value\":\"Morph\",\"total\":2},{\"value\":\"Relace\",\"total\":1},{\"value\":\"Inception\",\"total\":2},{\"value\":\"StepFun\",\"total\":1},{\"value\":\"Switchpoint\",\"total\":1},{\"value\":\"Nous Research\",\"total\":11},{\"value\":\"EleutherAI\",\"total\":1},{\"value\":\"NeverSleep\",\"total\":4},{\"value\":\"Gryphe\",\"total\":4},{\"value\":\"Deep Cogito\",\"total\":5},{\"value\":\"Anthracite\",\"total\":1},{\"value\":\"MiniMax\",\"total\":12},{\"value\":\"AlfredPros\",\"total\":1},{\"value\":\"Mancer\",\"total\":1},{\"value\":\"Alpindale\",\"total\":1},{\"value\":\"THUDM\",\"total\":1},{\"value\":\"OpenGVLab\",\"total\":1},{\"value\":\"Sao10K\",\"total\":9},{\"value\":\"Undi\",\"total\":2},{\"value\":\"rAIfle\",\"total\":1},{\"value\":\"TNG\",\"total\":6},{\"value\":\"TheDrummer\",\"total\":7},{\"value\":\"baai\",\"total\":3},{\"value\":\"deepseek-ai\",\"total\":3},{\"value\":\"ibm-granite\",\"total\":1},{\"value\":\"intfloat\",\"total\":3},{\"value\":\"kwaipilot\",\"total\":2},{\"value\":\"openrouter\",\"total\":1},{\"value\":\"prime-intellect\",\"total\":2},{\"value\":\"sentence-transformers\",\"total\":5},{\"value\":\"thenlper\",\"total\":2},{\"value\":\"venice\",\"total\":1}],\"total\":784},\"modalities\":{\"rows\":[{\"value\":\"audio\",\"total\":21},{\"value\":\"embeddings\",\"total\":23},{\"value\":\"file\",\"total\":80},{\"value\":\"image\",\"total\":224},{\"value\":\"text\",\"total\":1545},{\"value\":\"video\",\"total\":20}],\"total\":784},\"name\":{\"rows\":[{\"value\":\"gpt-oss-120b\",\"total\":20},{\"value\":\"Llama 3.3 70B Instruct\",\"total\":17},{\"value\":\"Qwen3 235B A22B Instruct 2507\",\"total\":17},{\"value\":\"Qwen3 Coder 480B A35B\",\"total\":17},{\"value\":\"gpt-oss-20b\",\"total\":17},{\"value\":\"DeepSeek V3 0324\",\"total\":15},{\"value\":\"GLM 4.6\",\"total\":14},{\"value\":\"R1 0528\",\"total\":13},{\"value\":\"Kimi K2 Thinking\",\"total\":12},{\"value\":\"DeepSeek V3.1\",\"total\":11},{\"value\":\"Kimi K2 0905\",\"total\":11},{\"value\":\"Qwen3 32B\",\"total\":10},{\"value\":\"Llama 3.1 8B Instruct\",\"total\":10},{\"value\":\"Qwen3 Next 80B A3B Instruct\",\"total\":9},{\"value\":\"Gemma 3 27B\",\"total\":8},{\"value\":\"Llama 4 Maverick\",\"total\":8},{\"value\":\"Qwen3 VL 235B A22B Instruct\",\"total\":8},{\"value\":\"Qwen3 30B A3B\",\"total\":8},{\"value\":\"MiniMax M2\",\"total\":8},{\"value\":\"Qwen2.5 VL 72B Instruct\",\"total\":7}],\"total\":784}}},\"prevCursor\":null,\"nextCursor\":50}],\"pageParams\":[{\"cursor\":null,\"size\":50}]},\"dataUpdateCount\":1,\"dataUpdatedAt\":1764340785584,\"error\":null,\"errorUpdateCount\":0,\"errorUpdatedAt\":0,\"fetchFailureCount\":0,\"fetchFailureReason\":null,\"fetchMeta\":null,\"isInvalidated\":false,\"status\":\"success\",\"fetchStatus\":\"idle\"},\"queryKey\":[\"models-data-table\",\"\"],\"queryHash\":\"[\\\"models-data-table\\\",\\\"\\\"]\"}]},\"children\":[[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"suppressHydrationWarning\":true,\"dangerouslySetInnerHTML\":{\"__html\":\"$18\"}}],[\"$\",\"div\",null,{\"className\":\"flex min-h-dvh w-full flex-col sm:flex-row pt-2 sm:p-0\",\"style\":{\"--total-padding-mobile\":\"calc(0.5rem + 0.5rem)\",\"--total-padding-desktop\":\"3rem\"},\"children\":[\"$\",\"$L19\",null,{}]}]]}]\n"])</script></body></html>